{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl_covid_path = Path(os.getcwd()).parent.parent.parent\n",
    "\n",
    "dir_data_interim = gpl_covid_path / 'data' / 'interim' / 'iran'\n",
    "dir_data_processed = gpl_covid_path / 'data' / 'processed'\n",
    "\n",
    "# Input\n",
    "path_iran_interim_adm0 = dir_data_interim / 'adm0' / 'IRN_interim.csv'\n",
    "path_iran_interim_adm2 = dir_data_interim / 'IRN_interim.csv'\n",
    "\n",
    "# Outputs\n",
    "path_iran_processed_adm0 = dir_data_processed / 'adm0' / 'IRN_processed.csv'\n",
    "path_iran_processed_adm2 = dir_data_processed / 'adm2' / 'IRN_processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm0_df = pd.read_csv(path_iran_interim_adm0, parse_dates=['date'])\n",
    "adm2_df = pd.read_csv(path_iran_interim_adm2, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean `adm2_df` and `adm0_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm0_df = adm0_df.drop(columns=['new_confirmed_cases', 'new_deaths_national'])\n",
    "adm2_df = adm2_df.drop(columns=['new_confirmed_cases', 'new_confirmed_cases_imputed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_non_monotonic_to_nan(array):\n",
    "    \"\"\"Converts a numpy array to a monotonically increasing one.\n",
    "    Args:\n",
    "        array (numpy.ndarray [N,]): input array\n",
    "    Returns:\n",
    "        numpy.ndarray [N,]: some values marked as missing, all non-missing\n",
    "            values should be monotonically increasing\n",
    "    Usage:\n",
    "        >>> convert_non_monotonic_to_nan(np.array([0, 0, 5, 3, 4, 6, 3, 7, 6, 7, 8]))\n",
    "        np.array([ 0.,  0., np.nan,  3., np.nan, np.nan,  3., np.nan,  6.,  7.,  8.])\n",
    "    \"\"\"\n",
    "    keep = np.arange(0, len(array))\n",
    "    is_monotonic = False\n",
    "    while not is_monotonic:\n",
    "        is_monotonic_array = np.hstack((\n",
    "            array[keep][1:] >= array[keep][:-1], np.array(True)))\n",
    "        is_monotonic = is_monotonic_array.all()\n",
    "        keep = keep[is_monotonic_array]\n",
    "    out_array = np.full_like(array.astype(np.float), np.nan)\n",
    "    out_array[keep] = array[keep]\n",
    "    return out_array\n",
    "\n",
    "def log_interpolate(array):\n",
    "    \"\"\"Interpolates assuming log growth.\n",
    "    Args:\n",
    "        array (numpy.ndarray [N,]): input array with missing values\n",
    "    Returns:\n",
    "        numpy.ndarray [N,]: all missing values will be filled\n",
    "    Usage:\n",
    "        >>> log_interpolate(np.array([0, np.nan, 2, np.nan, 4, 6, np.nan, 7, 8]))\n",
    "        np.array([0, 0, 2, 3, 4, 6, 7, 7, 8])\n",
    "    \"\"\"\n",
    "    idx = np.arange(0, len(array))\n",
    "    log_array = np.log(array.astype(np.float32) + 1e-1)\n",
    "    interp_array = np.interp(\n",
    "        x=idx, xp=idx[~np.isnan(array)], fp=log_array[~np.isnan(array)])\n",
    "    return np.round(np.exp(interp_array)).astype(np.int32)\n",
    "\n",
    "def impute_cumulative_array(array):\n",
    "    \"\"\"Ensures array is cumulative, imputing where necessary\n",
    "    Args:\n",
    "        array-like (numpy.ndarray [N,], pandas.Series, etc.): input array with missing values\n",
    "    Returns:\n",
    "        numpy.ndarray [N,]: all non-monotonic values will be filled by logarithmic interpolation\n",
    "    Usage:\n",
    "        >>> impute_cumulative_array(np.array([0, 0, 5, 3, 4, 6, 3, 7, 6, 7, 8]))\n",
    "        np.array([0, 0, 2, 3, 4, 6, 7, 7, 8])\n",
    "    \"\"\"\n",
    "    array = np.array(array)\n",
    "    array = convert_non_monotonic_to_nan(array)\n",
    "    array = log_interpolate(array)\n",
    "    return array\n",
    "\n",
    "def impute_cumulative_df(df, src_col, dst_col, groupby_col):\n",
    "    \"\"\"Calculates imputed columns and returns \n",
    "    Args:\n",
    "        df (pandas.DataFrame): input DataFrame with a cumulative column\n",
    "        src_col (str): name of cumulative column to impute\n",
    "        dst_col (str): name of imputed cumulative column\n",
    "        groupby_col (str): name of column containing names of administrative units,\n",
    "            values should correspond to groups whose values should be accumulating\n",
    "    Returns:\n",
    "        pandas.DataFrame: a copy of `df` with a newly imputed column specified by `dst_col`\n",
    "    Usage:\n",
    "        >>> impute_cumulative_df(pandas.DataFrame([[0, 'a'], [5, 'b'], [3, 'a'], [2, 'a'], [6, 'b']]), 0, 1)\n",
    "        pandas.DataFrame([[0, 'a', 0], [5, 'b', 5], [3, 'a', 0], [2, 'a', 2], [6, 'b', 6]], columns=[0, 1, 'imputed'])\n",
    "    \"\"\"\n",
    "    if src_col not in df.columns:\n",
    "        raise ValueError(f\"'{src_col}' not found\")\n",
    "    \n",
    "    if dst_col not in df.columns:\n",
    "        df[dst_col] = -1\n",
    "        \n",
    "    for adm_name in df[groupby_col].unique():\n",
    "        sub = df.loc[df[groupby_col] == adm_name].copy()\n",
    "        sub[dst_col] = impute_cumulative_array(sub[src_col])\n",
    "        df.loc[df[groupby_col] == adm_name] = sub\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_suffix = \"_imputed\"\n",
    "cumulative_prefix = \"cum_\"\n",
    "\n",
    "src_col = cumulative_prefix + 'confirmed_cases' + imputed_suffix\n",
    "dst_col = src_col\n",
    "adm2_df = impute_cumulative_df(adm2_df, src_col, dst_col, 'adm2_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_df = adm2_df.sort_values(['date', 'adm2_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = pd.read_csv(gpl_covid_path / 'data' / 'processed' / '[country]_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(adm0_df.columns) - set(template.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(adm2_df.columns) - set(template.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_df.to_csv(path_iran_processed_adm2, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm0_df.to_csv(path_iran_processed_adm0, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_jan_file = 'china_city_health_jan.xlsx'\n",
    "policy_file = 'china_city_policy.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean pre 01/24 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre 01/24 data\n",
    "df_jan = pd.read_excel(health_jan_file, sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process pre 1/24 data\n",
    "df_jan_merged = pd.DataFrame(\n",
    "    columns=['adm0_name', 'adm1_name', 'adm2_name', 'date'])\n",
    "for old_col, new_col in zip(\n",
    "    ['confirmed', 'death', 'recovery'],\n",
    "    ['cum_confirmed_cases', 'cum_deaths', 'cum_recoveries'],\n",
    "):\n",
    "    melted = df_jan[old_col].melt(\n",
    "        id_vars=['adm0_name', 'adm1_name', 'adm2_name'], \n",
    "        var_name='date',\n",
    "        value_name=new_col).dropna()\n",
    "    df_jan_merged = pd.merge(\n",
    "        df_jan_merged, melted,\n",
    "        how='outer', on=['adm0_name', 'adm1_name', 'adm2_name', 'date'])\n",
    "df_jan_merged = df_jan_merged.loc[df_jan_merged['adm2_name'] != 'Unknown', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean main data (scraped), harmonize city names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data downloaded from\n",
    "# https://github.com/BlankerL/DXY-COVID-19-Data\n",
    "# this will automatically upate when the data updates\n",
    "url = 'https://raw.githubusercontent.com/BlankerL/DXY-COVID-19-Data/master/csv/DXYArea.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop aggregates and cases in other countries\n",
    "df = df.loc[df['countryEnglishName'] == 'China', :]\n",
    "df = df.loc[df['cityName'].notna(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.describe(include='all')  # quick summary\n",
    "# df['provinceName'].unique()  # looks clean\n",
    "# df['provinceEnglishName'].unique()  # looks clean\n",
    "# df['cityName'].unique()  # looks messy, will keep raw data\n",
    "\n",
    "# # check unique English name for obs with the same Chinese cityName\n",
    "# for cn_name, group in df.groupby(['provinceName', 'cityName']):\n",
    "#     en_name = group['cityEnglishName'].unique()\n",
    "#     if len(en_name) > 1:\n",
    "#         print(cn_name)\n",
    "#         print(en_name)\n",
    "#         print(group['cityEnglishName'].shape)\n",
    "#         print(group['cityEnglishName'].value_counts())\n",
    "\n",
    "# # check all english city names\n",
    "# for en_name, _ in df.groupby(['provinceEnglishName', 'cityEnglishName']):\n",
    "#     print(en_name)\n",
    "\n",
    "# # check all chinese city names\n",
    "# for cn_name, _ in df.groupby(['provinceName', 'cityName']):\n",
    "#     print(cn_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and sort index\n",
    "df = df.set_index(['provinceName', 'cityName']).sort_index()\n",
    "# record notes\n",
    "df.loc[:, 'notes'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode city English names based on Chinese names\n",
    "cityEnglishName_dict = {\n",
    "    # 'provinceName', 'cityName': 'cityEnglishName', 'assignedToCity'\n",
    "    # for prisons\n",
    "    ('浙江省', '省十里丰监狱'): ('Shilifeng Prison', 'prison'),\n",
    "    ('山东省', '任城监狱'): ('Rencheng Prison', 'prison'),\n",
    "    ('湖北省', '监狱系统'): ('Prison', 'prison'),\n",
    "    # for harmonizing names\n",
    "    ('四川省', '凉山'): ('Liangshan Yi Autonomous Prefecture', np.nan),\n",
    "    ('四川省', '凉山州'): ('Liangshan Yi Autonomous Prefecture', np.nan),\n",
    "    # for imported cases\n",
    "    (None, '境外输入人员'): ('International Imported Cases', 'imported'),\n",
    "    (None, '外地来沪人员'): ('Domestic Imported Cases', 'imported'),\n",
    "    (None, '武汉来京人员'): ('Domestic Imported Cases', 'imported'),\n",
    "    (None, '外地来京人员'): ('Domestic Imported Cases', 'imported'),\n",
    "    (None, '外地来津'): ('Domestic Imported Cases', 'imported'),\n",
    "    (None, '外地来津人员'): ('Domestic Imported Cases', 'imported'),\n",
    "    (None, '外地来穗人员'): ('Domestic Imported Cases', 'imported'),\n",
    "    (None, '外地来粤人员'): ('Domestic Imported Cases', 'imported'),\n",
    "    # for unknown\n",
    "    (None, '待明确地区'): ('Unknown', 'unknown'),\n",
    "    (None, '未明确地区'): ('Unknown', 'unknown'),\n",
    "    (None, '未知'): ('Unknown', 'unknown'),\n",
    "    (None, '未知地区'): ('Unknown', 'unknown'),\n",
    "    (None, '不明地区'): ('Unknown', 'unknown'),\n",
    "    (None, '未明确地区'): ('Unknown', 'unknown'),\n",
    "    (None, '待明确'): ('Unknown', 'unknown'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up cityEnglishName\n",
    "for cn_name, values in cityEnglishName_dict.items():\n",
    "    cn_name = tuple(slice(s) if s is None else s for s in cn_name)\n",
    "    df.loc[cn_name, ['cityEnglishName', 'notes']] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check remaining missing values\n",
    "# df.loc[df['cityEnglishName'].isna(), :].index.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new admin level\n",
    "df.loc[:, 'adm3_name'] = 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode city English names based on Chinese names\n",
    "cityEnglishName_dict = {\n",
    "    ('上海市', '金山'): 'Jinshan District',\n",
    "    ('云南省', '红河'): 'Honghe',\n",
    "    ('云南省', '西双版纳州'): 'Xishuangbanna',\n",
    "    ('内蒙古自治区', '赤峰市松山区'): ('Chifeng', 'Songshan'),\n",
    "    ('内蒙古自治区', '赤峰市林西县'): ('Chifeng', 'Linxi'),\n",
    "    ('内蒙古自治区', '通辽市经济开发区'): 'Tongliao',\n",
    "    ('内蒙古自治区', '鄂尔多斯东胜区'): ('Ordos', 'Dongsheng'),\n",
    "    ('内蒙古自治区', '鄂尔多斯鄂托克前旗'): ('Ordos', 'Etuokeqianqi'),\n",
    "    ('内蒙古自治区', '锡林郭勒'): 'Xilingol League',\n",
    "    ('内蒙古自治区', '锡林郭勒盟'): 'Xilingol League',\n",
    "    ('内蒙古自治区', '锡林郭勒盟二连浩特'): ('Xilingol League', 'Erlianhaote'),\n",
    "    ('内蒙古自治区', '锡林郭勒盟锡林浩特'): ('Xilingol League', 'Xilinhaote'),\n",
    "    ('北京市', '石景山'): 'Shijingshan District',\n",
    "    ('北京市', '西城'): 'Xicheng District',\n",
    "    ('北京市', '通州'): 'Tongzhou District',\n",
    "    ('北京市', '门头沟'): 'Mentougou District',\n",
    "    ('北京市', '顺义'): 'Shunyi District',\n",
    "    ('新疆维吾尔自治区', '石河子'): 'Shihezi, Xinjiang Production and Construction Corps 8th Division',\n",
    "    ('新疆维吾尔自治区', '第七师'): 'Xinjiang Production and Construction Corps 7th Division',\n",
    "    ('新疆维吾尔自治区', '第九师'): 'Xinjiang Production and Construction Corps 9th Division',\n",
    "    ('新疆维吾尔自治区', '第八师'): 'Shihezi, Xinjiang Production and Construction Corps 8th Division',\n",
    "    ('新疆维吾尔自治区', '第八师石河子'): 'Shihezi, Xinjiang Production and Construction Corps 8th Division',\n",
    "    ('新疆维吾尔自治区', '第八师石河子市'): 'Shihezi, Xinjiang Production and Construction Corps 8th Division',\n",
    "    ('新疆维吾尔自治区', '第六师'): 'Xinjiang Production and Construction Corps 6th Division',\n",
    "    ('新疆维吾尔自治区', '胡杨河'): ('Xinjiang Production and Construction Corps 7th Division', 'Huyanghe'),\n",
    "    ('新疆维吾尔自治区', '阿克苏'): 'Akesu',\n",
    "    ('河北省', '邯郸市'): 'Handan',\n",
    "    ('河南省', '邓州'): 'Zhengzhou',\n",
    "    ('河南省', '长垣'): 'Changyuan',\n",
    "    ('河南省', '长垣县'): 'Changyuan',\n",
    "    ('河南省', '鹤壁市'): 'Hebi',\n",
    "    ('海南省', '陵水县'): 'Lingshui Li Autonomous County',\n",
    "    ('甘肃省', '白银市'): 'Baiyin',\n",
    "    ('甘肃省', '金昌市'): 'Jinchang',\n",
    "    ('重庆市', '石柱'): 'Shizhu Tujia Autonomous County',\n",
    "    ('重庆市', '秀山'): 'Xiushan Tujia and Miao Autonomous County',\n",
    "    ('重庆市', '酉阳'): 'Youyang Tujia and Miao Autonomous County',\n",
    "    ('青海省', '西宁市'): 'Xining',\n",
    "    # this is not missing but a typo in the original dataset\n",
    "    ('河南省', '邓州'): 'Dengzhou',\n",
    "    ('江苏省', '淮安'): \"Huai'an\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up cityEnglishName\n",
    "for cn_name, values in cityEnglishName_dict.items():\n",
    "    if isinstance(values, str):\n",
    "        df.loc[cn_name, 'cityEnglishName'] = values\n",
    "    elif len(values) == 2:\n",
    "        df.loc[cn_name, ['cityEnglishName', 'adm3_name']] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename variables\n",
    "df.rename({\n",
    "    'provinceEnglishName': 'adm1_name',\n",
    "    'cityEnglishName': 'adm2_name',\n",
    "    'city_confirmedCount': 'cum_confirmed_cases',\n",
    "    'city_deadCount': 'cum_deaths',\n",
    "    'city_curedCount': 'cum_recoveries',\n",
    "}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dates\n",
    "df.loc[:, 'updateTime'] = pd.to_datetime(df['updateTime'])\n",
    "df.loc[:, 'date'] = df['updateTime'].dt.date\n",
    "df.loc[:, 'date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the latest observation in each day\n",
    "df = df.sort_values(by=['updateTime'])\n",
    "df = df.drop_duplicates(subset=['adm1_name', 'adm2_name', 'adm3_name', 'date'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset columns\n",
    "df = df.loc[:, [\n",
    "    'adm1_name', 'adm2_name', 'adm3_name', 'date', 'notes',\n",
    "    'cum_confirmed_cases', 'cum_deaths', 'cum_recoveries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikaros/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2855: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  raw_cell, store_history, silent, shell_futures)\n"
     ]
    }
   ],
   "source": [
    "# for big cities, adjust adm level\n",
    "mask = df['adm1_name'].isin(['Shanghai', 'Beijing', 'Tianjin', 'Chongqing'])\n",
    "df.loc[mask, 'adm3_name'] = df.loc[mask, 'adm2_name'].tolist()\n",
    "df.loc[mask, 'adm2_name'] = df.loc[mask, 'adm1_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cases unassigned to cities\n",
    "df = df.loc[df['notes'] != 'prison', :]\n",
    "df = df.loc[\n",
    "    ~df['adm2_name'].isin(['International Imported Cases', 'Domestic Imported Cases', 'Unknown']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate to city level\n",
    "df = df.groupby(['adm1_name', 'adm2_name', 'date']).agg(\n",
    "    cum_confirmed_cases=pd.NamedAgg(\n",
    "        column='cum_confirmed_cases', aggfunc=np.nansum),\n",
    "    cum_deaths=pd.NamedAgg(\n",
    "        column='cum_deaths', aggfunc=np.nansum),\n",
    "    cum_recoveries=pd.NamedAgg(\n",
    "        column='cum_recoveries', aggfunc=np.nansum),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill adm0_name variable\n",
    "df.loc[:, 'adm0_name'] = 'CHN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with pre 01/24 data, create balanced panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with pre 1/24 data\n",
    "df = pd.concat([df, df_jan_merged], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createa balanced panel\n",
    "adm = df.loc[:, ['adm0_name', 'adm1_name', 'adm2_name']].drop_duplicates()\n",
    "days = pd.date_range(start='2020-01-10', end='2020-03-17')\n",
    "adm_days = pd.concat([adm.assign(date=d) for d in days])\n",
    "df = pd.merge(adm_days, df, how='left', on=['adm0_name', 'adm1_name', 'adm2_name', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill N/A for the first day\n",
    "df.loc[df['date'] == pd.Timestamp('2020-01-10'), :] = df.loc[df['date'] == pd.Timestamp('2020-01-10'), :].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# forward fill\n",
    "df = df.set_index(['adm0_name', 'adm1_name', 'adm2_name']).sort_index()\n",
    "for _, row in adm.iterrows():\n",
    "    df.loc[tuple(row), :] = df.loc[tuple(row), :].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate with JHU provincial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # validate with JHU\n",
    "# url = (\n",
    "#     'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/'\n",
    "#     'csse_covid_19_time_series/time_series_19-covid-Confirmed.csv')\n",
    "# jhu = pd.read_csv(url)\n",
    "# jhu = jhu.loc[jhu['Country/Region'] == 'China', :]\n",
    "# jhu = jhu.melt(\n",
    "#     id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], \n",
    "#     var_name='date',\n",
    "#     value_name='cum_confirmed_jhu')\n",
    "# jhu.loc[:, 'date'] = pd.to_datetime(jhu['date'])\n",
    "# jhu.set_index('Province/State', inplace=True)\n",
    "\n",
    "# # agg for visualization\n",
    "# df_viz = df.groupby(['adm0_name', 'adm1_name', 'date']).sum().reset_index().set_index(['adm1_name'])\n",
    "\n",
    "# # plot visualization\n",
    "# province_viz = 'Zhejiang'\n",
    "# fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# df_viz.loc[province_viz, :].plot(x='date', y='cum_confirmed_cases', ax=ax)\n",
    "# jhu.loc[province_viz, :].plot(x='date', y='cum_confirmed_jhu', ax=ax)\n",
    "# df_viz.loc[province_viz, :].plot(x='date', y='cum_recoveries', ax=ax)\n",
    "# df_viz.loc[province_viz, :].plot(x='date', y='cum_deaths', ax=ax)\n",
    "# ax.set_title(province_viz)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean policy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset of the policies in China\n",
    "df_policy = pd.read_excel(policy_file, sheet_name='City Policies')\n",
    "# subset columns\n",
    "df_policy = df_policy.loc[:, [\n",
    "    'adm0_name', 'adm1_name', 'adm2_name', 'date_start', 'date_end', 'policy']]\n",
    "# save set of policies\n",
    "policy_set = df_policy['policy'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('CHN', 'Jiangxi', 'ALL'), ('CHN', 'Anhui', 'ALL'), ('CHN', 'Hubei', 'ALL'), ('CHN', 'Liaoning', 'ALL')}\n"
     ]
    }
   ],
   "source": [
    "# check city name agreement\n",
    "policy_city_set = set(\n",
    "    df_policy.loc[:, ['adm0_name', 'adm1_name', 'adm2_name']].drop_duplicates()\n",
    "    .apply(tuple, axis=1).tolist())\n",
    "adm_city_set = set(\n",
    "    adm.drop_duplicates()\n",
    "    .apply(tuple, axis=1).tolist())\n",
    "print(policy_city_set - adm_city_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset adm1 policies\n",
    "adm1_policy = df_policy.loc[df_policy['adm2_name'] == 'ALL', :]\n",
    "# merge to create balanced panel\n",
    "adm1_policy = pd.merge(\n",
    "    adm, adm1_policy.drop(['adm2_name'], axis=1),\n",
    "    how='left', on=['adm0_name', 'adm1_name']).dropna(subset=['policy'])\n",
    "\n",
    "# subset adm2 policies\n",
    "adm2_policy = df_policy.loc[df_policy['adm2_name'] != 'ALL', :]\n",
    "\n",
    "# concat policies at different levels\n",
    "df_policy = pd.concat(\n",
    "    [adm1_policy, adm2_policy])\n",
    "\n",
    "# sort by date to discard duplicates\n",
    "df_policy = df_policy.sort_values(by=['date_start'])\n",
    "\n",
    "# drop duplicates\n",
    "df_policy = df_policy.drop_duplicates(\n",
    "    subset=['adm1_name', 'adm2_name', 'policy'], keep='first')\n",
    "\n",
    "# unstack to flip policy type to columns\n",
    "df_policy = df_policy.set_index(\n",
    "    ['adm0_name', 'adm1_name', 'adm2_name', 'policy']).unstack('policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to merge with multi index\n",
    "adm_days.set_index(['adm0_name', 'adm1_name', 'adm2_name'], inplace=True)\n",
    "adm_days.columns = pd.MultiIndex.from_tuples([('date', '')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge to create balanced panel\n",
    "df_policy = pd.merge(\n",
    "    adm_days, df_policy,\n",
    "    how='left', on=['adm0_name', 'adm1_name', 'adm2_name'])\n",
    "\n",
    "# fill N/As for dates\n",
    "df_policy = df_policy.fillna(pd.Timestamp('2021-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dummies\n",
    "for policy in policy_set:\n",
    "    df_policy.loc[:, (policy, '')] = (\n",
    "        (df_policy.loc[:, ('date', '')] >= df_policy.loc[:, ('date_start', policy)]) &\n",
    "        (df_policy.loc[:, ('date', '')] <= df_policy.loc[:, ('date_end', policy)])\n",
    "    )\n",
    "# discard intermediate variables\n",
    "df_policy = df_policy[['date'] + policy_set]\n",
    "# flatten the column index\n",
    "df_policy.columns = df_policy.columns.get_level_values(0)\n",
    "# convert data type\n",
    "df_policy.loc[:, policy_set] = df_policy.loc[:, policy_set].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_policy, how='inner', on=['adm0_name', 'adm1_name', 'adm2_name', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with testing policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with testing policies\n",
    "# source:\n",
    "# https://english.kyodonews.net/news/2020/02/6982cc1e130f-china-records-2-straight-days-of-fewer-than-1000-new-covid-19-cases.html\n",
    "# https://www.worldometers.info/coronavirus/how-to-interpret-feb-12-case-surge/\n",
    "df.loc[:, 'testing_regime'] = (\n",
    "    (df['date'] >= pd.Timestamp('2020-02-13')).astype(int) +\n",
    "    (df['date'] >= pd.Timestamp('2020-02-20')).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe(include='all')  # looks fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple sanity checks, Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_non_monotonic_to_nan(array):\n",
    "    \"\"\"Converts a numpy array to a monotonically increasing one.\n",
    "    \n",
    "    Args:\n",
    "        array (numpy.ndarray [N,]): input array\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray [N,]: some values marked as missing, all non-missing\n",
    "            values should be monotonically increasing\n",
    "    \n",
    "    Usage:\n",
    "        >>> convert_non_monotonic_to_nan(np.array([0, 0, 5, 3, 4, 6, 3, 7, 6, 7, 8]))\n",
    "        np.array([ 0.,  0., np.nan,  3., np.nan, np.nan,  3., np.nan,  6.,  7.,  8.])\n",
    "    \"\"\"\n",
    "    keep = np.arange(0, len(array))\n",
    "    is_monotonic = False\n",
    "    while not is_monotonic:\n",
    "        is_monotonic_array = np.hstack((\n",
    "            array[keep][1:] >= array[keep][:-1], np.array(True)))\n",
    "        is_monotonic = is_monotonic_array.all()\n",
    "        keep = keep[is_monotonic_array]\n",
    "    out_array = np.full_like(array.astype(np.float), np.nan)\n",
    "    out_array[keep] = array[keep]\n",
    "    return out_array\n",
    "\n",
    "\n",
    "def log_interpolate(array):\n",
    "    \"\"\"Interpolates assuming log growth.\n",
    "\n",
    "    Args:\n",
    "        array (numpy.ndarray [N,]): input array with missing values\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray [N,]: all missing values will be filled\n",
    "\n",
    "    Usage:\n",
    "        >>> log_interpolate(np.array([0, np.nan, 2, np.nan, 4, 6, np.nan, 7, 8]))\n",
    "        np.array([0, 0, 2, 3, 4, 6, 7, 7, 8])\n",
    "    \"\"\"\n",
    "    idx = np.arange(0, len(array))\n",
    "    log_array = np.log(array.astype(np.float32) + 1e-1)\n",
    "    interp_array = np.interp(\n",
    "        x=idx, xp=idx[~np.isnan(array)], fp=log_array[~np.isnan(array)])\n",
    "    return np.round(np.exp(interp_array)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop/impute non monotonic observations\n",
    "for col in ['cum_confirmed_cases', 'cum_deaths', 'cum_recoveries']:\n",
    "    for _, row in adm.iterrows():\n",
    "        df.loc[tuple(row), col] = convert_non_monotonic_to_nan(\n",
    "            df.loc[tuple(row), col].values)\n",
    "        df.loc[tuple(row), col + '_imputed'] = log_interpolate(\n",
    "            df.loc[tuple(row), col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add active cases\n",
    "df.loc[:, 'active_cases'] = (\n",
    "    df.loc[:, 'cum_confirmed_cases'].values -\n",
    "    df.loc[:, 'cum_deaths'].values -\n",
    "    df.loc[:, 'cum_recoveries'].values\n",
    ")\n",
    "df.loc[:, 'active_cases_imputed'] = (\n",
    "    df.loc[:, 'cum_confirmed_cases_imputed'].values -\n",
    "    df.loc[:, 'cum_deaths_imputed'].values -\n",
    "    df.loc[:, 'cum_recoveries_imputed'].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add city id\n",
    "df = pd.merge(\n",
    "    df, adm.assign(adm2_id=range(adm.shape[0])),\n",
    "    how='left', on=['adm0_name', 'adm1_name', 'adm2_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('CHN_processed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import date\n",
    "import pyproj\n",
    "from census import Census\n",
    "import requests\n",
    "import lxml.html as lh\n",
    "import numpy as np\n",
    "\n",
    "from src import utils as cutil\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "datestamp = '20200315'\n",
    "sacredentials_fpath = '/Users/ianbolliger/service-accounts/bolliger32.json'\n",
    "\n",
    "adm1_shp_dir = cutil.DATA_RAW / 'multi_country' / f'ne_10m_admin_1_states_provinces_{datestamp}'\n",
    "adm3_url_fmt = 'https://biogeo.ucdavis.edu/data/gadm3.6/gpkg/gadm36_{iso3}_gpkg.zip'\n",
    "adm3_gpkg_fmt = 'gadm36_{iso3}.gpkg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpkg_path(iso3):\n",
    "    return cutil.get_adm3_dir(iso3, datestamp) / adm3_gpkg_fmt.format(iso3=iso3)\n",
    "\n",
    "def download_and_extract(url,out_dir):\n",
    "    if not out_dir.is_dir():\n",
    "        with urlopen(url) as zipresp:\n",
    "            with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "                zfile.extractall(out_dir)\n",
    "            \n",
    "def process_gadm(in_gdf, iso3):\n",
    "    cols_to_load = ['NAME_1','NAME_2','geometry']\n",
    "    col_map = {\n",
    "        'NAME_0': 'adm0_name',\n",
    "        'NAME_1':'adm1_name',\n",
    "        'NAME_2':'adm2_name'\n",
    "    }\n",
    "    if 'NAME_3' in in_gdf.columns:\n",
    "        cols_to_load.append('NAME_3')\n",
    "        col_map['NAME_3'] = 'adm3_name'\n",
    "        \n",
    "    in_gdf = in_gdf[cols_to_load]\n",
    "    in_gdf = in_gdf.rename(columns=col_map)\n",
    "\n",
    "    cent = in_gdf['geometry'].centroid\n",
    "    in_gdf['latitude'] = cent.y\n",
    "    in_gdf['longitude'] = cent.x\n",
    "    in_gdf['adm0_name'] = iso3\n",
    "    \n",
    "    in_gdf = in_gdf.set_index(['adm0_name','adm1_name','adm2_name'])\n",
    "    if 'adm3_name' in in_gdf.columns:\n",
    "        in_gdf = in_gdf.set_index('adm3_name', append=True)\n",
    "    \n",
    "    return in_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global adm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file\n",
    "adm1_url = 'https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip'\n",
    "download_and_extract(adm1_url, adm1_shp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process\n",
    "in_gdf = gpd.read_file(adm1_shp_dir)\n",
    "adm_gdf = in_gdf[['adm0_a3','name','geometry', 'latitude','longitude','gadm_level']]\n",
    "adm_gdf = adm_gdf.rename(columns={\n",
    "    'adm0_a3':'adm0_name',\n",
    "    'name': 'adm1_name'\n",
    "}).set_index(['adm0_name','adm1_name','gadm_level'])\n",
    "\n",
    "# for now, when there are duplicates, just drop the second one without any better information\n",
    "# could not find a data dictionary for the shapefile\n",
    "adm_gdf = adm_gdf[~adm_gdf.index.duplicated(keep='first')].reset_index(drop=False, level='gadm_level')\n",
    "\n",
    "# we know france is actually admin 2\n",
    "adm_gdf.loc[idx['FRA',:],'gadm_level']=2\n",
    "\n",
    "# separate into levels\n",
    "adm1_gdf = adm_gdf[adm_gdf.gadm_level==1].drop(columns='gadm_level')\n",
    "adm2_gdf = adm_gdf[adm_gdf.gadm_level==2].drop(columns='gadm_level')\n",
    "adm2_gdf.index = adm2_gdf.index.set_names('adm2_name',level='adm1_name')\n",
    "\n",
    "# Set up an adm3 dataset that is currently empty\n",
    "adm3_gdf = gpd.GeoDataFrame(columns = adm2_gdf.reset_index(drop=False).columns, crs=adm_gdf.crs)\n",
    "adm3_gdf['adm3_name'] = []\n",
    "adm3_gdf['adm1_name'] = []\n",
    "adm3_gdf = adm3_gdf.set_index(['adm0_name','adm1_name','adm2_name','adm3_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adm2+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_fr_fpath = cutil.DATA / 'interim' / 'france' / 'departement_info.dta'\n",
    "adm2_fr = pd.read_stata(adm2_fr_fpath, index_col='departement_name', columns=['departement_name','adm1_name','cheflieu','densitehabitantskm2', 'superficiekmâ', 'population'])\n",
    "adm2_fr.index = adm2_fr.index.str.encode('ISO-8859-1').str.decode('utf-8')\n",
    "adm2_fr.cheflieu = adm2_fr.cheflieu.str.encode('ISO-8859-1').str.decode('utf-8')\n",
    "adm2_fr.index.name = 'adm2_name'\n",
    "adm2_fr = adm2_fr.rename(columns={\n",
    "    \"cheflieu\": \"capital\",\n",
    "    \"densitehabitantskm2\":\"pop_density_km2\",\n",
    "    \"superficiekmâ\": \"area_km2\"\n",
    "})\n",
    "\n",
    "# manually correct some differences in naming btwn 2 datasets\n",
    "name_map = {\n",
    "    \"Guyane française\": \"Guyane\",\n",
    "    \"Haute-Rhin\": \"Haut-Rhin\",\n",
    "    \"Vendée\": \"Vandée\",\n",
    "    \"Côtes-d'Armor\": \"Côtes d'Armor\",\n",
    "    \"Seine-Saint-Denis\": \"Seine-St-Denis\",\n",
    "    \"Val-d'Oise\": \"Val-D'Oise\",\n",
    "    \"Seien-et-Marne\": \"Seine-et-Marne\"\n",
    "}\n",
    "adm2_gdf = adm2_gdf.rename(index=name_map, level='adm2_name')\n",
    "\n",
    "# merge back in\n",
    "adm2_gdf = adm2_gdf.join(adm2_fr, on='adm2_name', how='outer').reset_index(drop=False).set_index(['adm0_name','adm1_name','adm2_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these are from the same source but:\n",
    "- some work with the gpkg file others with the shapefile\n",
    "- some are adm3 some are adm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "isos = ['ITA','USA','CHN','KOR','IRN']\n",
    "\n",
    "for iso3 in isos:\n",
    "    # sometimes the shapefile has finer resolution, sometimes the gpkg file does...\n",
    "    try:\n",
    "        download_and_extract(adm3_url_fmt.format(iso3=iso3), cutil.get_adm3_dir(iso3, datestamp))\n",
    "        in_gdf = process_gadm(gpd.read_file(get_gpkg_path(iso3)), iso3)\n",
    "    except KeyError:\n",
    "        download_and_extract(adm3_url_fmt.format(iso3=iso3).replace('gpkg','shp'), cutil.get_adm3_dir(iso3, datestamp))\n",
    "        in_gdf = process_gadm(gpd.read_file(cutil.get_adm3_dir(iso3, datestamp)), iso3)\n",
    "        \n",
    "    if 'adm3_name' in in_gdf.index.names:\n",
    "        adm3_gdf = adm3_gdf.append(in_gdf)\n",
    "        \n",
    "        # now aggregate to level 2 to insert into that level\n",
    "        in_gdf = in_gdf.dissolve(by=['adm0_name','adm1_name','adm2_name'])\n",
    "        in_gdf['latitude'] = in_gdf.geometry.centroid.y\n",
    "        in_gdf['longitude'] = in_gdf.geometry.centroid.x\n",
    "    \n",
    "    # insert into level 2 dataset\n",
    "    assert not iso3 in adm2_gdf.index.get_level_values('adm0_name').unique()\n",
    "    adm2_gdf = adm2_gdf.append(in_gdf)\n",
    "    \n",
    "    # now aggregate to level 1 to replace that level with better/more consistent data\n",
    "    in_gdf = in_gdf.dissolve(by=['adm0_name','adm1_name'])\n",
    "    in_gdf['latitude'] = in_gdf.geometry.centroid.y\n",
    "    in_gdf['longitude'] = in_gdf.geometry.centroid.x\n",
    "    adm1_gdf = adm1_gdf[adm1_gdf.index.get_level_values('adm0_name')!=iso3]\n",
    "    adm1_gdf = adm1_gdf.append(in_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual name adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some manual adjustments to make this match with the naming of the data produced by country teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_dict = {\n",
    "    'Emilia-Romagna':'Emilia Romagna',\n",
    "    'Friuli-Venezia Giulia':'Friuli Venezia Giulia',\n",
    "    'Apulia':'Puglia',\n",
    "    'Sicily':'Sicilia',\n",
    "}\n",
    "add_regions = ['P.A. Bolzano', 'P.A. Trento']\n",
    "add_regions_prov = [i.replace('P.A. ','') for i in add_regions]\n",
    "province_dict = {\n",
    "    \"Forli' - Cesena\":'Forlì-Cesena',\n",
    "    \"Reggio Nell'Emilia\":\"Reggio nell'Emilia\",\n",
    "    \"Padua\":\"Padova\",\n",
    "    \"Reggio Di Calabria\":\"Reggio di Calabria\",\n",
    "    \"Pesaro E Urbino\":\"Pesaro e Urbino\",\n",
    "    \"Syracuse\":\"Siracusa\",\n",
    "    \"Florence\":\"Firenze\",\n",
    "    \"Mantua\":\"Mantova\",\n",
    "    \"Monza and Brianza\":\"Monza e della Brianza\",\n",
    "}\n",
    "add_provinces = [\"Sud Sardegna\"]\n",
    "add_provinces_reg = ['Sardegna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new regions/provinces\n",
    "n_reg = len(add_regions)\n",
    "new_reg = pd.DataFrame(dict(\n",
    "    adm0_name=['ITA']*n_reg,\n",
    "    adm1_name=add_regions,\n",
    "    adm2_name=add_regions_prov\n",
    ")).set_index(['adm0_name','adm1_name','adm2_name'])\n",
    "n_prov = len(add_provinces)\n",
    "new_prov = pd.DataFrame(dict(\n",
    "    adm0_name=['ITA']*n_prov,\n",
    "    adm2_name=add_provinces,\n",
    "    adm1_name=add_provinces_reg\n",
    ")).set_index(['adm0_name','adm1_name','adm2_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_gdf = adm1_gdf.rename(index=region_dict, level='adm1_name')\n",
    "adm2_gdf = adm2_gdf.rename(index=region_dict, level='adm1_name')\n",
    "adm3_gdf = adm3_gdf.rename(index=region_dict, level='adm1_name')\n",
    "adm2_gdf = adm2_gdf.rename(index=province_dict, level='adm2_name')\n",
    "adm3_gdf = adm3_gdf.rename(index=province_dict, level='adm2_name')\n",
    "\n",
    "for i in [new_reg,new_prov]:\n",
    "    adm1_gdf = adm1_gdf.append(i.reset_index(level='adm2_name',drop=True))\n",
    "    adm2_gdf = adm2_gdf.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_apikey = '24f4f2dc127d1386d07db9af73526aa052c9c41f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Census(census_apikey)\n",
    "pop_city = pd.DataFrame(c.acs5.state_place(('NAME', 'B01003_001E'),Census.ALL,Census.ALL))\n",
    "pop_cty = pd.DataFrame(c.acs5.state_county(('NAME', 'B01003_001E'),Census.ALL,Census.ALL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Place-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the place-level populations\n",
    "pop_city[['adm3_name','adm_1_name']] = pd.DataFrame(pop_city.NAME.str.split(', ').values.tolist(), index= pop_city.index)\n",
    "pop_city = pop_city.rename(columns={'B01003_001E':'pop'}).drop(columns='NAME')\n",
    "pop_city = pop_city.set_index(['adm3_name','adm_1_name'])\n",
    "pop_city.to_csv(cutil.DATA / 'interim' / 'usa' / 'adm3_pop.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### County-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get county-level populations\n",
    "hasc_fips_url='http://www.statoids.com/yus.html'\n",
    "#Create a handle, page, to handle the contents of the website\n",
    "page = requests.get(hasc_fips_url)\n",
    "\n",
    "#Store the contents of the website under doc\n",
    "doc = lh.fromstring(page.content)\n",
    "\n",
    "#Parse data\n",
    "tr_elements = doc.xpath('//*[@id=\"yui-main\"]/div/div/pre/text()[1]')\n",
    "row_list = tr_elements[0].split(\"\\r\\n\")[1:-1]\n",
    "headers = row_list[0].split()\n",
    "valid_rows = [r for r in row_list if r != \"\" and r[:4] not in [\"Name\",\"----\"]]\n",
    "name = [r[:23].rstrip() for r in valid_rows]\n",
    "t = [r[23] for r in valid_rows]\n",
    "hasc = [r[25:33] for r in valid_rows]\n",
    "fips = [r[34:39] for r in valid_rows]\n",
    "pop = [int(r[40:49].lstrip().replace(',','')) for r in valid_rows]\n",
    "area_km2 = [int(r[50:57].lstrip().replace(',','')) for r in valid_rows]\n",
    "area_mi2 = [int(r[58:65].lstrip().replace(',','')) for r in valid_rows]\n",
    "z = [r[66] for r in valid_rows]\n",
    "capital = [r[68:] for r in valid_rows]\n",
    "\n",
    "# turn into dataframe\n",
    "us_county_df = pd.DataFrame({\n",
    "    'name': name,\n",
    "    'type': t,\n",
    "    'hasc': hasc,\n",
    "    'fips': fips,\n",
    "    'population': pop,\n",
    "    'area_km2': area_km2,\n",
    "    'capital': capital\n",
    "}).set_index('hasc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge in us adm2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_gdf = gpd.read_file(get_gpkg_path('USA'))\n",
    "us_gdf = us_gdf[us_gdf.HASC_2.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_pops = us_gdf.join(us_county_df, on='HASC_2', how='left')\n",
    "us_pops = us_pops[['NAME_1','NAME_2','fips','population','area_km2','capital']]\n",
    "us_pops = us_pops.rename(columns={'NAME_1':'adm1_name','NAME_2':'adm2_name'})\n",
    "us_pops['pop_density_km2'] = us_pops['population'] / us_pops['area_km2']\n",
    "us_pops['adm0_name'] = 'USA'\n",
    "us_pops = us_pops.set_index(['adm0_name','adm1_name','adm2_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge back into global adm datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this for France as well, b/c we haven't merged in adm2 pops to adm1 for france yet either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_gdf = adm2_gdf.fillna(us_pops)\n",
    "st_pops = adm2_gdf.loc[:,'population'].groupby(['adm0_name','adm1_name']).sum(min_count=1)\n",
    "adm1_gdf['population'] = st_pops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pop data is copied directly from `GPL_covid/data/raw/italy/italy_policy.gsheet:Population`, which is from Google Public Data. None of the pops are comprehensive for that administrative level, so we will not be aggregating and applying to the higher-up level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_str = \"\"\"\n",
    "adm1\tPopulation (2018)\tadm2\tPopulation (2018)\tadm3\tPopulation (2018)\n",
    "Lombardy\t10036300\tLodi\t229765\tBertonico\t1127\n",
    "Veneto\t4905000\tPadua\t936740\tCasalpusterlengo\t15280\n",
    "Marche\t1531800\tPesaro and Urbino\t360125\tCastelgerundo\t1489\n",
    "Liguria\t1557000\tSavona\t277810\tCastiglione d'Adda\t4651\n",
    "Piedmont\t4375900\tAlessandria\t424174\tCodogno\t15901\n",
    "Emilia-Romagna\t4452600\tAsti\t215884\tFombio\t2325\n",
    "Campania\t5826900\tModena\t701896\tMaleo\t3133\n",
    "Sicilia\t5027000\tNovara\t369595\tSan Fiorano\t1841\n",
    "Friuli Venezia Giulia\t1215500\tParma\t450256\tSomaglia\t3797\n",
    "Abruzzo\t1315200\tPiacenza\t286781\tTerranova dei Passerini\t918\n",
    "Apulia\t4048200\tReggio nell'Emilia\t532575\tVo'Eugane\t3341\n",
    "\t\tRimini\t337325\tTaranto\t198283\n",
    "\t\tVerbano-Cusio-Ossola\t159159\tMessina\t234293\n",
    "\t\tVercelli\t172307\t\t\n",
    "\t\tNapoli\t3101000\t\t\n",
    "\t\tPalermo\t1260200\t\t\n",
    "\t\tTaranto\t580319\t\t\n",
    "\t\tMessina\t631297\t\t\n",
    "\"\"\"\n",
    "\n",
    "ita_pop_2_maps = {\n",
    "    'Pesaro and Urbino': 'Pesaro e Urbino',\n",
    "    'Padua': \"Padova\"\n",
    "}\n",
    "\n",
    "ita_pop_1_maps = {\n",
    "    'Lombardy': 'Lombardia',\n",
    "    'Piedmont': \"Piemonte\",\n",
    "    'Emilia-Romagna':'Emilia Romagna',\n",
    "    'Apulia':'Puglia',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_data = [i.split(\"\\t\") for i in pop_str.strip().split(\"\\n\")]\n",
    "pop_df = pd.DataFrame(pop_data[1:],columns=pop_data[0]).rename(columns={'Population (2018)':'population'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_pop_3 = pop_df.iloc[:,4:]\n",
    "ita_pop_3 = ita_pop_3[((ita_pop_3.notnull()) & (ita_pop_3!='')).all(axis=1)].rename(columns={'adm3':'adm3_name'})\n",
    "ita_pop_3.population = ita_pop_3.population.astype(int)\n",
    "ita_pop_3['adm0_name'] = 'ITA'\n",
    "ita_pop_3 = ita_pop_3.set_index(['adm0_name','adm3_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm3_gdf = adm3_gdf.rename(lambda x: x.replace(\n",
    "    \"d' Adda\", \"d'Adda\").replace(\n",
    "    \"Terranova Dei Passerini\", \"Terranova dei Passerini\"), level='adm3_name')\n",
    "\n",
    "# these two municipalities merged\n",
    "castel = gpd.GeoDataFrame(adm3_gdf.loc[idx[:,:,:,['Cavacurta','Camairago']],['geometry']]).dissolve(by=['adm0_name','adm1_name','adm2_name'])\n",
    "castel['adm3_name'] = ['Castelgerundo']\n",
    "castel['latitude'] = castel.geometry.centroid.y\n",
    "castel['longitude'] = castel.geometry.centroid.x\n",
    "castel = castel.set_index('adm3_name', append=True)\n",
    "adm3_gdf = adm3_gdf[~adm3_gdf.index.get_level_values('adm3_name').isin(['Cavacurta','Camairago'])].append(castel)\n",
    "\n",
    "# this municipality not in dataset\n",
    "adm3_gdf.loc[idx['ITA','Veneto','Padua',\"Vo'Eugane\"],:] = pd.Series({\n",
    "    'geometry': None,\n",
    "    'latitude': np.nan,\n",
    "    'longitude': np.nan\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm3_gdf = adm3_gdf.join(ita_pop_3, on=['adm0_name','adm3_name'],how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_pop_2 = pop_df.iloc[:,2:4]\n",
    "ita_pop_2 = ita_pop_2[ita_pop_2.notnull().all(axis=1)].rename(columns={'adm2':'adm2_name'})\n",
    "ita_pop_2.population = ita_pop_2.population.astype(int)\n",
    "ita_pop_2['adm0_name'] = 'ITA'\n",
    "ita_pop_2.adm2_name = ita_pop_2.adm2_name.apply(lambda x: ita_pop_2_maps[x] if x in ita_pop_2_maps.keys() else x)\n",
    "ita_pop_2 = ita_pop_2.set_index(['adm0_name','adm2_name']).population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pop = pd.DataFrame(adm2_gdf.reset_index(level='adm1_name', drop=False).population.fillna(ita_pop_2))\n",
    "new_pop['adm1_name'] = adm2_gdf.index.get_level_values('adm1_name')\n",
    "new_pop = new_pop.reset_index(drop=False).set_index(['adm0_name','adm1_name','adm2_name'])\n",
    "adm2_gdf.population = new_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_pop_1 = pop_df.iloc[:,:2]\n",
    "ita_pop_1 = ita_pop_1[(ita_pop_1!=\"\").any(axis=1)].rename(columns={'adm1':'adm1_name'})\n",
    "ita_pop_1.population = ita_pop_1.population.astype(int)\n",
    "ita_pop_1.adm1_name = ita_pop_1.adm1_name.apply(lambda x: ita_pop_1_maps[x] if x in ita_pop_1_maps.keys() else x)\n",
    "ita_pop_1 = ita_pop_1.set_index('adm1_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_pop_1 = pop_df.iloc[:,:2]\n",
    "ita_pop_1 = ita_pop_1[(ita_pop_1!=\"\").any(axis=1)].rename(columns={'adm1':'adm1_name'})\n",
    "ita_pop_1.population = ita_pop_1.population.astype(int)\n",
    "ita_pop_1.adm1_name = ita_pop_1.adm1_name.apply(lambda x: ita_pop_1_maps[x] if x in ita_pop_1_maps.keys() else x)\n",
    "ita_pop_1['adm0_name'] = 'ITA'\n",
    "ita_pop_1 = ita_pop_1.set_index(['adm0_name','adm1_name'])\n",
    "\n",
    "adm1_gdf.population = adm1_gdf.population.fillna(ita_pop_1.population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finishing_touches(df):\n",
    "    # area\n",
    "    area_km2_mercator = df[df.geometry.notna()].to_crs('EPSG:3395').geometry.area / 1e6\n",
    "    if 'area_km2' in df.columns:\n",
    "        df['area_km2'] = df.area_km2.fillna(area_km2_mercator)\n",
    "    else:\n",
    "        df['area_km2'] = area_km2_mercator\n",
    "        \n",
    "    # pop density\n",
    "    if 'pop_density_km2' in df.columns:\n",
    "        df.pop_density_km2 = df.pop_density_km2.fillna(df.population.astype(float) / df.area_km2)\n",
    "    else:\n",
    "        df['pop_density_km2'] = df.population.astype(float) / df.area_km2\n",
    "    \n",
    "    # lat/lon\n",
    "    df.longitude = df.longitude.fillna(df.geometry.centroid.x)\n",
    "    df.latitude = df.latitude.fillna(df.geometry.centroid.y)\n",
    "    \n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "adm1_gdf = finishing_touches(adm1_gdf)\n",
    "adm2_gdf = finishing_touches(adm2_gdf)\n",
    "adm3_gdf = finishing_touches(adm3_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,i in enumerate([adm1_gdf,adm2_gdf,adm3_gdf]):\n",
    "    fname = f'adm{ix+1}'\n",
    "    out_dir = cutil.DATA_INTERIM / 'adm' / fname\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    i.to_file(out_dir / f'{fname}.shp', index=True)\n",
    "    i.drop(columns='geometry').to_csv(out_dir / f'{fname}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

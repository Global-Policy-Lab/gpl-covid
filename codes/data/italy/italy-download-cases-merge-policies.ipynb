{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Italy health (epi) and policy data\n",
    "- Download and process health data from GitHub at the regional (`adm1`) and provincial (`adm2`) levels\n",
    "- Clean, standardize, and impute health data\n",
    "- Merge population data\n",
    "- Merge collected policies\n",
    "- Save outputs at `data/processed/adm1/ITA_processed.csv` and `data/processed/adm2/ITA_processed.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "end_of_analysis_date = '2020-03-18'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root of project\n",
    "dir_gpl_covid = Path(os.getcwd()).parent.parent.parent\n",
    "\n",
    "dir_data = dir_gpl_covid / 'data'\n",
    "\n",
    "# Administrative unit names\n",
    "path_adm1 = dir_data / 'interim' / 'adm' / 'adm1' / 'adm1.csv'\n",
    "path_adm2 = dir_data / 'interim' / 'adm' / 'adm2' / 'adm2.csv'\n",
    "\n",
    "# Template for processed dataset (output of this notebook)\n",
    "path_template = dir_data / 'processed' / '[country]_processed.csv'\n",
    "\n",
    "dir_italy_raw = dir_gpl_covid / 'data' / 'raw' / 'italy'\n",
    "dir_italy_interim = dir_gpl_covid / 'data' / 'interim' / 'italy'\n",
    "dir_processed = dir_gpl_covid / 'data' / 'processed'\n",
    "\n",
    "policies_date = \"20200318\"\n",
    "\n",
    "# Inputs\n",
    "# CSV form of policies Google sheet\n",
    "path_italy_policies = dir_italy_raw / f'italy_policy_static_{policies_date}.csv'\n",
    "url_adm2_cases = \"https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-province/dpc-covid19-ita-province.csv\"\n",
    "url_adm1_cases = \"https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv\"\n",
    "\n",
    "# Outputs\n",
    "## Intermediate outputs\n",
    "path_italy_raw_province = dir_italy_raw / 'italy-cases-by-province.csv'\n",
    "path_italy_raw_region = dir_italy_raw / 'italy-cases-by-region.csv'\n",
    "path_italy_interim_province = dir_italy_interim / 'italy-cases-by-province.csv'\n",
    "path_italy_interim_region = dir_italy_interim / 'italy-cases-by-region.csv'\n",
    "\n",
    "## Final outputs\n",
    "path_processed_region = dir_processed / 'adm1' / 'ITA_processed.csv'\n",
    "path_processed_province = dir_processed / 'adm2' / 'ITA_processed.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read raw data from Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Administrative unit names\n",
    "adm1_df = pd.read_csv(path_adm1)\n",
    "adm2_df = pd.read_csv(path_adm2)\n",
    "\n",
    "# Columns in template (i.e. columns allowed in output)\n",
    "template_cols = set(pd.read_csv(path_template).columns)\n",
    "\n",
    "# Italy-specific data\n",
    "adm2_cases = pd.read_csv(url_adm2_cases)\n",
    "adm1_cases = pd.read_csv(url_adm1_cases)\n",
    "\n",
    "policies_full = pd.read_csv(path_italy_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save raw case data from URL to project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_cases.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_cases.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_cases.to_csv(path_italy_raw_province, index=False)\n",
    "adm1_cases.to_csv(path_italy_raw_region, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate and clean health data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Settings\n",
    "Affixes defined in `data_dictionary.gsheet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_prefix = 'cum_'\n",
    "imputed_suffix = '_imputed'\n",
    "popweighted_suffix = '_popwt'\n",
    "optional_suffix = '_opt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate field names from Italian to project naming scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names based on table descriptions here: https://github.com/pcm-dpc/COVID-19\n",
    "replace_dict = {\n",
    "    'data':'date',\n",
    "    'lat':'lat',\n",
    "    'long':'lon',\n",
    "    'stato':'adm0_name',\n",
    "    'denominazione_regione':'adm1_name',\n",
    "    'denominazione_provincia':'adm2_name',\n",
    "    'codice_regione':'adm1_id',\n",
    "    'codice_provincia':'adm2_id',\n",
    "    'sigla_provincia':'province_abbrev',\n",
    "    'totale_attualmente_positivi':'active_cases',\n",
    "    'nuovi_attualmente_positivi':'active_cases_new',\n",
    "    'totale_casi':cumulative_prefix + 'confirmed_cases',\n",
    "    'ricoverati_con_sintomi':cumulative_prefix + 'hospitalized_symptom',\n",
    "    'terapia_intensiva':cumulative_prefix + 'intensive_care',\n",
    "    'totale_ospedalizzati':cumulative_prefix + 'hospitalized',\n",
    "    'isolamento_domiciliare':cumulative_prefix + 'home_confinement',\n",
    "    'dimessi_guariti': cumulative_prefix + 'recoveries',\n",
    "    'deceduti': cumulative_prefix + 'deaths',\n",
    "    'totale_casi': cumulative_prefix + 'confirmed_cases',\n",
    "    'tamponi':cumulative_prefix + 'tests',\n",
    "}\n",
    "\n",
    "adm2_cases = adm2_cases.rename(columns=replace_dict)\n",
    "adm1_cases = adm1_cases.rename(columns=replace_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_datetime(dates):\n",
    "    return pd.to_datetime(dates.str[:10])\n",
    "\n",
    "adm2_cases['date'] = extract_date_from_datetime(adm2_cases['date'])\n",
    "adm1_cases['date'] = extract_date_from_datetime(adm1_cases['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean lat-lon coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_cases.loc[:,['lat','lon']] = adm2_cases.loc[:,['lat','lon']].replace(0, np.nan)\n",
    "assert adm1_cases['lat'].isnull().sum() == 0\n",
    "assert adm1_cases['lon'].isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean unknown province names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"In fase di definizione/aggiornamento\" translates to \"Being defined / updated\". These observations are dropped from the final output\n",
    "adm2_cases['adm2_name'] = adm2_cases['adm2_name'].replace(\n",
    "    'In fase di definizione/aggiornamento', 'Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unnecessary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_cases = adm2_cases.drop(columns=['province_abbrev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute cumulative confirmed cases at `adm2` level on the first day of the dataset (2/24/2020) from `adm1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adm1 totals computed by grouping on Adm1 in the Adm2 dataset\n",
    "adm1_cases_from_provinces = adm2_cases.groupby(['date', 'adm1_name'])['cum_confirmed_cases'].sum()\n",
    "\n",
    "# Compute cumulative cases in the Adm1 dataset by mapping to totals from Adm2 dataset\n",
    "def get_province_total(region_row):\n",
    "    return adm1_cases_from_provinces.loc[region_row['date'], region_row['adm1_name']]\n",
    "\n",
    "# This sum should match each adm1-level total for each day, except the first day in the dataset\n",
    "adm1_cases['cum_confirmed_cases_prov'] = adm1_cases.apply(get_province_total, axis=1)\n",
    "\n",
    "# Compute DataFrame mapping adm1 names to first-day case totals that are missing in `adm2_cases`\n",
    "day1_cases = adm1_cases[adm1_cases['cum_confirmed_cases_prov'] != adm1_cases['cum_confirmed_cases']][['adm1_name', 'cum_confirmed_cases']].set_index('adm1_name')\n",
    "\n",
    "# Mask to fill in adm2 rows with missing day 1 cum_confirmed_cases\n",
    "replace_day1_mask = (\n",
    "    (adm2_cases['adm1_name'].isin(day1_cases.index)) & \n",
    "    (adm2_cases['date'] == '2020-02-24') & \n",
    "    (adm2_cases['adm2_name'] == 'Unknown')\n",
    ")\n",
    "\n",
    "# Set cum_confirmed_cases of \"Unknown\" adm2 rows to each corresponding adm1 total on day 1\n",
    "adm2_cases.loc[replace_day1_mask, 'cum_confirmed_cases'] = adm2_cases.loc[replace_day1_mask, 'adm1_name'].apply(lambda x: day1_cases.loc[x])\n",
    "\n",
    "# Remove helper col\n",
    "adm1_cases = adm1_cases.drop(columns=['cum_confirmed_cases_prov'])\n",
    "\n",
    "# Check that all regions with positive cases on day 1 are accounted for\n",
    "adm2_cases[(adm2_cases['adm1_name'].isin(day1_cases.index)) & (adm2_cases['date'] == '2020-02-24') & (adm2_cases['adm2_name'] == 'Unknown')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Check data limitations\n",
    "Go to https://github.com/pcm-dpc/COVID-19 and check \"Avvisi\" for any documented data issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill known missing cumulative totals as nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on missingness gathered from GitHub \"Avvisi\" section\n",
    "adm1_days_missing = [\n",
    "    (\"2020-03-10\", \"Lombardia\"),\n",
    "    (\"2020-03-11\", \"Abruzzo\"),\n",
    "    (\"2020-03-16\", \"P.A. Trento\"),\n",
    "    (\"2020-03-16\", \"Puglia\"),\n",
    "    (\"2020-03-18\", \"Campania\"),\n",
    "]\n",
    "\n",
    "adm2_days_missing = [\n",
    "    (\"2020-03-17\", \"Rimini\"),\n",
    "    (\"2020-03-18\", \"Parma\")\n",
    "]\n",
    "\n",
    "# Replace missing values in `adm_cases` to null. These missing values are tabulated in the source data\n",
    "# as the value of that variable on the previous non-missing day, which can skew analysis of growth rates\n",
    "def fill_missing_as_null(adm_cases, date, adm_name, adm_col):\n",
    "    \n",
    "    # Get all cumulative columns\n",
    "    cum_cols = [col for col in adm_cases.columns if 'cum_' in col]\n",
    "    \n",
    "    # Replace values known to be missing with np.nan\n",
    "    for col in cum_cols:\n",
    "        adm_cases.loc[(\n",
    "            (adm_cases['date'] == date) & (adm_cases[adm_col] == adm_name)\n",
    "        ), col] = np.nan\n",
    "        \n",
    "    return adm_cases\n",
    "\n",
    "# Fill in nulls for missing adm1 data, in the adm1 dataset\n",
    "for date, adm1 in adm1_days_missing:\n",
    "    adm1_cases = fill_missing_as_null(adm1_cases, date, adm1, 'adm1_name')\n",
    "        \n",
    "# Fill in nulls for missing adm1 data, in the adm2 dataset\n",
    "for date, adm1 in adm1_days_missing:\n",
    "    adm2_cases = fill_missing_as_null(adm2_cases, date, adm1, 'adm1_name')\n",
    "\n",
    "# Fill in nulls for missing adm2 data, in the adm2 dataset\n",
    "for date, adm2 in adm2_days_missing:\n",
    "    adm2_cases = fill_missing_as_null(adm2_cases, date, adm2, 'adm2_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_cases[adm1_cases['cum_confirmed_cases'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_cases[adm2_cases['cum_confirmed_cases'].isnull()].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute values in cases where cumulative counts rise and then fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_non_monotonic_to_nan(array):\n",
    "    \"\"\"Converts a numpy array to a monotonically increasing one.\n",
    "    Args:\n",
    "        array (numpy.ndarray [N,]): input array\n",
    "    Returns:\n",
    "        numpy.ndarray [N,]: some values marked as missing, all non-missing\n",
    "            values should be monotonically increasing\n",
    "    Usage:\n",
    "        >>> convert_non_monotonic_to_nan(np.array([0, 0, 5, 3, 4, 6, 3, 7, 6, 7, 8]))\n",
    "        np.array([ 0.,  0., np.nan,  3., np.nan, np.nan,  3., np.nan,  6.,  7.,  8.])\n",
    "    \"\"\"\n",
    "    keep = np.arange(0, len(array))\n",
    "    is_monotonic = False\n",
    "    while not is_monotonic:\n",
    "        is_monotonic_array = np.hstack((\n",
    "            array[keep][1:] >= array[keep][:-1], np.array(True)))\n",
    "        is_monotonic = is_monotonic_array.all()\n",
    "        keep = keep[is_monotonic_array]\n",
    "    out_array = np.full_like(array.astype(np.float), np.nan)\n",
    "    out_array[keep] = array[keep]\n",
    "    return out_array\n",
    "\n",
    "def log_interpolate(array):\n",
    "    \"\"\"Interpolates assuming log growth.\n",
    "    Args:\n",
    "        array (numpy.ndarray [N,]): input array with missing values\n",
    "    Returns:\n",
    "        numpy.ndarray [N,]: all missing values will be filled\n",
    "    Usage:\n",
    "        >>> log_interpolate(np.array([0, np.nan, 2, np.nan, 4, 6, np.nan, 7, 8]))\n",
    "        np.array([0, 0, 2, 3, 4, 6, 7, 7, 8])\n",
    "    \"\"\"\n",
    "    idx = np.arange(0, len(array))\n",
    "    log_array = np.log(array.astype(np.float32) + 1e-1)\n",
    "    interp_array = np.interp(\n",
    "        x=idx, xp=idx[~np.isnan(array)], fp=log_array[~np.isnan(array)])\n",
    "    return np.round(np.exp(interp_array)).astype(np.int32)\n",
    "\n",
    "def impute_cumulative_array(array):\n",
    "    \"\"\"Ensures array is cumulative, imputing where necessary\n",
    "    Args:\n",
    "        array-like (numpy.ndarray [N,], pandas.Series, etc.): input array with missing values\n",
    "    Returns:\n",
    "        numpy.ndarray [N,]: all non-monotonic values will be filled by logarithmic interpolation\n",
    "    Usage:\n",
    "        >>> impute_cumulative_array(np.array([0, 0, 5, 3, 4, 6, 3, 7, 6, 7, 8]))\n",
    "        np.array([0, 0, 2, 3, 4, 6, 7, 7, 8])\n",
    "    \"\"\"\n",
    "    array = np.array(array)\n",
    "    \n",
    "    # Hold onto original array to retrieve null values later\n",
    "    array_orig = array.copy()\n",
    "    \n",
    "    # Convert array to re-impute nulls as filled by previous day\n",
    "    array = np.array(pd.Series(array_orig).fillna(method='ffill'))\n",
    "    \n",
    "    # Replace cumulative totals that rise and then fall with nulls, assuming latest information is most correct\n",
    "    array = convert_non_monotonic_to_nan(array)\n",
    "    \n",
    "    # Keep nulls from original array and nulls from checking for monotonicity\n",
    "    array = np.where(np.isnan(array_orig), np.nan, array)\n",
    "    \n",
    "    # Interpolate all nulls\n",
    "    array = log_interpolate(array)\n",
    "    \n",
    "    return array\n",
    "\n",
    "def impute_cumulative_df(df, src_col, dst_col, groupby_col):\n",
    "    \"\"\"Calculates imputed columns and returns \n",
    "    Args:\n",
    "        df (pandas.DataFrame): input DataFrame with a cumulative column\n",
    "        src_col (str): name of cumulative column to impute\n",
    "        dst_col (str): name of imputed cumulative column\n",
    "        groupby_col (str): name of column containing names of administrative units,\n",
    "            values should correspond to groups whose values should be accumulating\n",
    "    Returns:\n",
    "        pandas.DataFrame: a copy of `df` with a newly imputed column specified by `dst_col`\n",
    "    Usage:\n",
    "        >>> impute_cumulative_df(pandas.DataFrame([[0, 'a'], [5, 'b'], [3, 'a'], [2, 'a'], [6, 'b']]), 0, 1)\n",
    "        pandas.DataFrame([[0, 'a', 0], [5, 'b', 5], [3, 'a', 0], [2, 'a', 2], [6, 'b', 6]], columns=[0, 1, 'imputed'])\n",
    "    \"\"\"\n",
    "    if dst_col not in df.columns:\n",
    "        df[dst_col] = -1\n",
    "\n",
    "    for adm_name in df[groupby_col].unique():\n",
    "        sub = df.loc[df[groupby_col] == adm_name].copy()\n",
    "        sub[dst_col] = impute_cumulative_array(sub[src_col])\n",
    "        \n",
    "        # Set rising-then-falling cumulative counts to null in the original column\n",
    "        sub.loc[sub[src_col].notnull(), src_col] = convert_non_monotonic_to_nan(\n",
    "            np.array(sub.loc[sub[src_col].notnull(), src_col])\n",
    "        )\n",
    "        \n",
    "        df.loc[df[groupby_col] == adm_name] = sub\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute all cumulative totals in imputed column, fill as null where cumulative totals fall in source column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute adm2 cumulative total (just one)\n",
    "adm2_cases = impute_cumulative_df(\n",
    "    adm2_cases, \n",
    "    cumulative_prefix + 'confirmed_cases', \n",
    "    cumulative_prefix + 'confirmed_cases' + imputed_suffix, \n",
    "    'adm2_id')\n",
    "\n",
    "# Impute any cumulative totals in adm1\n",
    "adm1_cases_cum_cols = [col for col in adm1_cases.columns if 'cum_' in col]\n",
    "for src_col in adm1_cases_cum_cols:\n",
    "    dst_col = src_col + imputed_suffix\n",
    "    adm1_cases = impute_cumulative_df(adm1_cases, src_col, dst_col, 'adm1_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save processed health data to `interim` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_cases.to_csv(path_italy_interim_province, index=False)\n",
    "adm1_cases.to_csv(path_italy_interim_region, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Health with Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies_full['Date'] = pd.to_datetime(policies_full['Date'])\n",
    "policies_full['Policy'] = policies_full['Policy'].str.strip()\n",
    "\n",
    "# Convert 'Opt' to indicator variable\n",
    "policies_full['Opt'] = policies_full['Opt'].replace({\"Y\":1, \"N\":0})\n",
    "policies_full['Opt'] = policies_full['Opt'].fillna(0)\n",
    "\n",
    "# Set default values for null fields\n",
    "policies_full['adm0_name'] = policies_full['adm0_name'].fillna('Italy')\n",
    "policies_full['adm1_affected'] = policies_full['adm1_affected'].fillna('All')\n",
    "policies_full['adm2_affected'] = policies_full['adm2_affected'].fillna('All')\n",
    "policies_full['adm3_affected'] = policies_full['adm3_affected'].fillna('All')\n",
    "\n",
    "# Check that population weights are all there\n",
    "assert len(policies_full[policies_full['adm1_pop_weight_perc_affected'].isnull()]) == 0\n",
    "assert len(policies_full[policies_full['adm2_pop_weight_perc_affected'].isnull()]) == 0\n",
    "\n",
    "# Remove any duplicates, grouping on relevant columns\n",
    "policies = policies_full[\n",
    "    ['adm3_affected','adm2_pop_weight_perc_affected','adm2_affected', \n",
    "     'adm1_pop_weight_perc_affected','adm1_affected','adm0_name',\n",
    "     'Date','Policy','Opt']\n",
    "].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this fails, have to implement `testing_regime` as categorical variable\n",
    "# This works right now because only one change in \"testing_regime\", a categorical variable\n",
    "assert policies.groupby('Policy')['Policy'].count()['testing_regime'] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `home_isolation_partial` as `home_isolation` with a relative weight of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies['home_isolation_partial'] = False\n",
    "policies.loc[policies['Policy'] == 'home_isolation_partial', 'home_isolation_partial'] = True\n",
    "\n",
    "policies.loc[policies['Policy'] == 'home_isolation_partial', 'adm2_pop_weight_perc_affected'] = policies.loc[policies['Policy'] == 'home_isolation_partial', 'adm2_pop_weight_perc_affected'] * 0.5\n",
    "policies.loc[policies['Policy'] == 'home_isolation_partial', 'adm1_pop_weight_perc_affected'] = policies.loc[policies['Policy'] == 'home_isolation_partial', 'adm1_pop_weight_perc_affected'] * 0.5\n",
    "policies['Policy'] = policies['Policy'].replace('home_isolation_partial', 'home_isolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace optional policies with `policy_name` to `policy_name_opt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies.loc[policies['Opt'] == 1, 'Policy'] = policies.loc[policies['Opt'] == 1, 'Policy'] + optional_suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure all policies listed have corresponding adm-units in health data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map some regions/provinces in policy dataset to corresponding names in health data\n",
    "replace_dict = {\n",
    "    'Lombardy':'Lombardia',\n",
    "    'Piedmont':'Piemonte',\n",
    "    'Emilia-Romagna':'Emilia Romagna',\n",
    "    'Padua':'Padova',\n",
    "    'Venice':'Venezia',\n",
    "    'Pesaro and Urbino':'Pesaro e Urbino',\n",
    "    'Apulia':'Puglia', \n",
    "}\n",
    "\n",
    "# Standardize naming between policy and health data\n",
    "policies['adm1_affected'] = policies['adm1_affected'].replace(replace_dict)\n",
    "policies['adm2_affected'] = policies['adm2_affected'].replace(replace_dict)\n",
    "\n",
    "adm1_not_found = set(policies['adm1_affected'].unique()) - set(adm1_cases['adm1_name'].unique()) - set(['All'])\n",
    "adm2_not_found = set(policies['adm2_affected'].unique()) - set(adm2_cases['adm2_name'].unique()) - set(['All'])\n",
    "\n",
    "assert len(adm1_not_found) == 0\n",
    "assert len(adm2_not_found) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get population of autonomous provinces, coded as regions and provinces in health data, from region data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bolzano appears in the regions and provinces datasets\n",
    "adm2_df.loc[adm2_df['adm1_name'] == 'P.A. Bolzano', 'population'] = (\n",
    "    adm1_df.loc[adm1_df['adm1_name'] == 'P.A. Bolzano', 'population'].to_numpy()[0]\n",
    ")\n",
    "\n",
    "# Trento appears in the regions and provinces datasets\n",
    "adm2_df.loc[adm2_df['adm1_name'] == 'P.A. Trento', 'population'] = (\n",
    "    adm1_df.loc[adm1_df['adm1_name'] == 'P.A. Trento', 'population'].to_numpy()[0]\n",
    ")\n",
    "\n",
    "# Trentino-Alto Adige is redundant here since Bolzano + Trento == Trentino-Alto Adige\n",
    "adm1_df = adm1_df.loc[adm1_df['adm1_name'] != 'Trentino-Alto Adige']\n",
    "adm2_df = adm2_df.loc[adm2_df['adm1_name'] != 'Trentino-Alto Adige']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign populations by adm-unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_pops = adm1_df.loc[adm1_df['adm0_name'] == 'ITA'].set_index('adm1_name')['population']\n",
    "\n",
    "adm2_pops = adm2_df.loc[adm2_df['adm0_name'] == 'ITA'].set_index('adm2_name')['population']\n",
    "\n",
    "adm1_cases['population'] = adm1_cases['adm1_name'].apply(\n",
    "    lambda adm1: adm1_pops.loc[adm1] if adm1 in adm1_pops else np.nan\n",
    ")\n",
    "\n",
    "adm2_cases['population'] = adm2_cases['adm2_name'].apply(\n",
    "    lambda adm2: adm2_pops.loc[adm2] if adm2 in adm2_pops else np.nan\n",
    ")\n",
    "\n",
    "assert set(adm2_cases.loc[adm2_cases['population'].isnull(), 'adm2_name']) == set(['Unknown'])\n",
    "assert adm1_cases['population'].isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret `home_isolation_partial` as `home_isolation` with a 1/2 multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_policies = pd.DataFrame(policies.groupby(['Date', 'adm1_affected', 'Policy', 'Opt', 'home_isolation_partial'])['adm1_pop_weight_perc_affected'].sum()).reset_index()\n",
    "adm2_policies = pd.DataFrame(policies.groupby(['Date', 'adm1_affected', 'adm2_affected', 'Policy', 'Opt', 'home_isolation_partial'])['adm2_pop_weight_perc_affected'].sum()).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign policy indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_from_popweights = ['testing_regime', 'travel_ban_intl_in', 'travel_ban_intl_out']\n",
    "\n",
    "# Initialize policy columns in health data\n",
    "for policy_name in policies['Policy'].unique():\n",
    "    adm1_cases[policy_name] = 0\n",
    "    adm2_cases[policy_name] = 0\n",
    "    \n",
    "    # Include pop-weighted column for policies applied within the country\n",
    "    if policy_name not in exclude_from_popweights:\n",
    "        adm1_cases[policy_name + popweighted_suffix] = 0\n",
    "        adm2_cases[policy_name + popweighted_suffix] = 0\n",
    "\n",
    "def assign_policy_variables(adm_cases, policy_on_mask, policy, partial, perc_affected):\n",
    "    # Policies originally coded as 'home_isolation_partial' get 0.5 weight as 'home_isolation'\n",
    "    policy_on_value = 1 if not (policy == 'home_isolation' and partial == True) else 0.5\n",
    "    \n",
    "    adm_cases.loc[policy_on_mask, policy] = policy_on_value\n",
    "    \n",
    "    if policy not in exclude_from_popweights:\n",
    "        adm_cases.loc[policy_on_mask, policy + popweighted_suffix] = perc_affected\n",
    "    \n",
    "    return adm_cases\n",
    "        \n",
    "for date, policy, optional, adm, perc_affected, partial in adm1_policies[\n",
    "    ['Date', 'Policy', 'Opt', 'adm1_affected', 'adm1_pop_weight_perc_affected', 'home_isolation_partial']\n",
    "].to_numpy():\n",
    "    \n",
    "    # All policies on or after policy was enacted, where one of these conditions applies:\n",
    "        # The policy applies to all Adm1\n",
    "        # This Adm1 is named explicitly\n",
    "    policy_on_mask = (\n",
    "        (adm1_cases['date'] >= date) &\n",
    "        (\n",
    "            (adm1_cases['adm1_name'] == adm) | (adm == 'All')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    adm1_cases = assign_policy_variables(adm1_cases, policy_on_mask, policy, partial, perc_affected)\n",
    "    \n",
    "for date, policy, optional, adm1, adm2, perc_affected, partial in adm2_policies[\n",
    "    ['Date', 'Policy', 'Opt', 'adm1_affected', 'adm2_affected', 'adm2_pop_weight_perc_affected', 'home_isolation_partial']\n",
    "].to_numpy():\n",
    "    \n",
    "    # All policies on or after policy was enacted, where one of these conditions applies:\n",
    "        # The policy applies to all Adm1\n",
    "        # This Adm2 is named explicitly\n",
    "        # This Adm2's Adm1 is named explicitly, and Adm2 is listed as \"All\" (i.e. all under that Adm1)\n",
    "    policy_on_mask = (\n",
    "        (adm2_cases['date'] >= date) &\n",
    "        (\n",
    "            (adm2_cases['adm2_name'] == adm2) | \n",
    "            (adm1 == 'All') | \n",
    "            (\n",
    "                (adm2 == 'All') & (adm1 == adm2_cases['adm1_name'])\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    adm2_cases = assign_policy_variables(adm2_cases, policy_on_mask, policy, partial, perc_affected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of policies in each health-policy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_cases['policies_enacted'] = 0\n",
    "adm2_cases['policies_enacted'] = 0\n",
    "for policy_name in policies['Policy'].unique():\n",
    "    adm1_cases['policies_enacted'] += adm1_cases[policy_name]\n",
    "    adm2_cases['policies_enacted'] += adm2_cases[policy_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use default value for `no_gathering_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_cases['no_gathering_size'] = 0\n",
    "adm2_cases['no_gathering_size'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read `[country]_processed` template and find any output columns missing in template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = pd.read_csv(path_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_from_template = (set(adm1_cases.columns) | set(adm2_cases.columns)) - set(template.columns)\n",
    "assert len(missing_from_template) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter dataset to rows on or before cutoff date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_cases = adm1_cases[adm1_cases['date'] <= end_of_analysis_date]\n",
    "adm2_cases = adm2_cases[adm2_cases['date'] <= end_of_analysis_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out rows where adm1 is known but adm2 is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_cases = adm2_cases[adm2_cases['adm2_name'] != 'Unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to `ITA_processed.csv`'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_cases.to_csv(path_processed_region, index=False)\n",
    "adm2_cases.to_csv(path_processed_province, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

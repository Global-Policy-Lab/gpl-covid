{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Iran health (epi) and policy data\n",
    "- Open interim Iran dataset\n",
    "- Clean, standardize, and impute health data\n",
    "- Merge populations\n",
    "- Save outputs at `data/processed/adm0/IRN_processed.csv` and `data/processed/adm2/IRN_processed.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "end_of_analysis_date = '2020-03-18'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project directory\n",
    "dir_gpl_covid = Path(os.getcwd()).parent.parent.parent\n",
    "\n",
    "dir_data_interim = dir_gpl_covid / 'data' / 'interim' / 'iran'\n",
    "dir_data_processed = dir_gpl_covid / 'data' / 'processed'\n",
    "dir_adm_pop = dir_gpl_covid / 'data' / 'interim' / 'adm'\n",
    "\n",
    "# Input\n",
    "path_iran_interim_adm0 = dir_data_interim / 'adm0' / 'IRN_interim.csv'\n",
    "path_iran_interim_adm1 = dir_data_interim / 'IRN_interim.csv'\n",
    "path_pop_adm1 = dir_adm_pop / 'adm1' / 'adm1.csv'\n",
    "path_template = dir_gpl_covid / 'data' / 'processed' / '[country]_processed.csv'\n",
    "\n",
    "# Outputs\n",
    "path_iran_processed_adm0 = dir_data_processed / 'adm0' / 'IRN_processed.csv'\n",
    "path_iran_processed_adm1 = dir_data_processed / 'adm1' / 'IRN_processed.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read interim datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm0_df = pd.read_csv(path_iran_interim_adm0, parse_dates=['date'])\n",
    "adm1_df = pd.read_csv(path_iran_interim_adm1, parse_dates=['date'])\n",
    "\n",
    "# Population data\n",
    "adm1_pop_df = pd.read_csv(path_pop_adm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean `adm1_df` and `adm0_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename `adm2` to `adm1` (correct previous coding error), remove old `adm1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_df = adm1_df.drop(columns=['adm1_name'])\n",
    "adm1_df = adm1_df.rename(columns={'adm2_name':'adm1_name'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unnecessary columns (these totals are accounted for in `cum_` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm0_df = adm0_df.drop(columns=['new_confirmed_cases', 'new_deaths_national'])\n",
    "adm1_df = adm1_df.drop(columns=['new_confirmed_cases', 'new_confirmed_cases_imputed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_df = adm1_df.sort_values(['date', 'adm1_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge in population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_pop_iran = adm1_pop_df.loc[adm1_pop_df['adm0_name'] == 'IRN'].copy()\n",
    "\n",
    "# Standardize province names to `adm1.csv`\n",
    "replace_dict = {\n",
    "    'Alburz': 'Alborz',\n",
    "    'Chaharmahal.and.Bakhtiari': 'Chahar Mahall and Bakhtiari',\n",
    "    'East.Azerbaijan': 'East Azarbaijan',\n",
    "    'Hamedan': 'Hamadan',\n",
    "    'Khuzistan': 'Khuzestan',\n",
    "    'Kohgiluyeh.and.Boyer_Ahmad': 'Kohgiluyeh and Buyer Ahmad',\n",
    "    'Kurdistan': 'Kordestan',\n",
    "    'North.Khorasan': 'North Khorasan',\n",
    "    'Razavi.Khorasan': 'Razavi Khorasan',\n",
    "    'Sistan.and.Baluchestan': 'Sistan and Baluchestan',\n",
    "    'South.Khorasan': 'South Khorasan',\n",
    "    'West.Azerbaijan': 'West Azarbaijan'\n",
    "}\n",
    "\n",
    "# Create adm1 population Series\n",
    "adm1_pops = adm1_pop_iran.set_index('adm1_name')['population']\n",
    "\n",
    "# Replace province names with standardized versions\n",
    "adm1_df['adm1_name'] = adm1_df['adm1_name'].replace(replace_dict)\n",
    "\n",
    "# Assign population data\n",
    "adm1_df['population'] = adm1_df['adm1_name'].apply(\n",
    "    lambda adm1: adm1_pops.loc[adm1] if adm1 in adm1_pops else np.nan\n",
    ")\n",
    "\n",
    "# Make sure no population tallies are missing\n",
    "assert adm1_df['population'].isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define imputation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_non_monotonic_to_nan(array):\n",
    "    \"\"\"Converts a numpy array to a monotonically increasing one.\n",
    "    Args:\n",
    "        array (numpy.ndarray [N,]): input array\n",
    "    Returns:\n",
    "        numpy.ndarray [N,]: some values marked as missing, all non-missing\n",
    "            values should be monotonically increasing\n",
    "    Usage:\n",
    "        >>> convert_non_monotonic_to_nan(np.array([0, 0, 5, 3, 4, 6, 3, 7, 6, 7, 8]))\n",
    "        np.array([ 0.,  0., np.nan,  3., np.nan, np.nan,  3., np.nan,  6.,  7.,  8.])\n",
    "    \"\"\"\n",
    "    keep = np.arange(0, len(array))\n",
    "    is_monotonic = False\n",
    "    while not is_monotonic:\n",
    "        is_monotonic_array = np.hstack((\n",
    "            array[keep][1:] >= array[keep][:-1], np.array(True)))\n",
    "        is_monotonic = is_monotonic_array.all()\n",
    "        keep = keep[is_monotonic_array]\n",
    "    out_array = np.full_like(array.astype(np.float), np.nan)\n",
    "    out_array[keep] = array[keep]\n",
    "    return out_array\n",
    "\n",
    "def log_interpolate(array):\n",
    "    \"\"\"Interpolates assuming log growth.\n",
    "    Args:\n",
    "        array (numpy.ndarray [N,]): input array with missing values\n",
    "    Returns:\n",
    "        numpy.ndarray [N,]: all missing values will be filled\n",
    "    Usage:\n",
    "        >>> log_interpolate(np.array([0, np.nan, 2, np.nan, 4, 6, np.nan, 7, 8]))\n",
    "        np.array([0, 0, 2, 3, 4, 6, 7, 7, 8])\n",
    "    \"\"\"\n",
    "    idx = np.arange(0, len(array))\n",
    "    log_array = np.log(array.astype(np.float32) + 1e-1)\n",
    "    interp_array = np.interp(\n",
    "        x=idx, xp=idx[~np.isnan(array)], fp=log_array[~np.isnan(array)])\n",
    "    return np.round(np.exp(interp_array)).astype(np.int32)\n",
    "\n",
    "def impute_cumulative_array(array):\n",
    "    \"\"\"Ensures array is cumulative, imputing where necessary\n",
    "    Args:\n",
    "        array-like (numpy.ndarray [N,], pandas.Series, etc.): input array with missing values\n",
    "    Returns:\n",
    "        numpy.ndarray [N,]: all non-monotonic values will be filled by logarithmic interpolation\n",
    "    Usage:\n",
    "        >>> impute_cumulative_array(np.array([0, 0, 5, 3, 4, 6, 3, 7, 6, 7, 8]))\n",
    "        np.array([0, 0, 2, 3, 4, 6, 7, 7, 8])\n",
    "    \"\"\"\n",
    "    array = np.array(array).copy()\n",
    "    array = convert_non_monotonic_to_nan(array)\n",
    "    array = log_interpolate(array)\n",
    "    return array\n",
    "\n",
    "def impute_cumulative_df(df, src_col, dst_col, groupby_col):\n",
    "    \"\"\"Calculates imputed columns and returns \n",
    "    Args:\n",
    "        df (pandas.DataFrame): input DataFrame with a cumulative column\n",
    "        src_col (str): name of cumulative column to impute\n",
    "        dst_col (str): name of imputed cumulative column\n",
    "        groupby_col (str): name of column containing names of administrative units,\n",
    "            values should correspond to groups whose values should be accumulating\n",
    "    Returns:\n",
    "        pandas.DataFrame: a copy of `df` with a newly imputed column specified by `dst_col`\n",
    "    Usage:\n",
    "        >>> impute_cumulative_df(pandas.DataFrame([[0, 'a'], [5, 'b'], [3, 'a'], [2, 'a'], [6, 'b']]), 0, 1)\n",
    "        pandas.DataFrame([[0, 'a', 0], [5, 'b', 5], [3, 'a', 0], [2, 'a', 2], [6, 'b', 6]], columns=[0, 1, 'imputed'])\n",
    "    \"\"\"\n",
    "    if src_col not in df.columns:\n",
    "        raise ValueError(f\"'{src_col}' not found\")\n",
    "    \n",
    "    if dst_col not in df.columns:\n",
    "        df[dst_col] = -1\n",
    "        \n",
    "    for adm_name in df[groupby_col].unique():\n",
    "        sub = df.loc[df[groupby_col] == adm_name].copy()\n",
    "        sub[dst_col] = impute_cumulative_array(sub[src_col])\n",
    "        \n",
    "        # Replace non-monotonic values in original `cum_confirmed_cases` column with nulls\n",
    "        raw_cum_col = 'cum_confirmed_cases'\n",
    "        sub.loc[sub[raw_cum_col].notnull(), raw_cum_col] = convert_non_monotonic_to_nan(\n",
    "            np.array(sub.loc[sub[raw_cum_col].notnull(), raw_cum_col])\n",
    "        )\n",
    "        \n",
    "        df.loc[df[groupby_col] == adm_name] = sub\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute cumulative confirmed cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_suffix = \"_imputed\"\n",
    "cumulative_prefix = \"cum_\"\n",
    "\n",
    "src_col = cumulative_prefix + 'confirmed_cases' + imputed_suffix\n",
    "dst_col = src_col\n",
    "adm1_df = impute_cumulative_df(adm1_df, src_col, dst_col, 'adm1_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all columns are in template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = pd.read_csv(path_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(adm0_df.columns) - set(template.columns)) == 0\n",
    "assert len(set(adm1_df.columns) - set(template.columns)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that date does not extend past end of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert adm1_df['date'].max() <= pd.to_datetime(end_of_analysis_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output to `IRN_processed.csv` datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm0_df.to_csv(path_iran_processed_adm0, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_df.to_csv(path_iran_processed_adm1, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

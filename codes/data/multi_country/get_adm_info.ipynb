{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import date\n",
    "import pyproj\n",
    "from census import Census\n",
    "import requests\n",
    "import lxml.html as lh\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import shutil\n",
    "\n",
    "from codes import utils as cutil\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "datestamp = \"20200320\"\n",
    "sacredentials_fpath = \"/Users/ianbolliger/service-accounts/bolliger32.json\"\n",
    "\n",
    "adm1_shp_path = (\n",
    "    cutil.DATA_RAW\n",
    "    / \"multi_country\"\n",
    "    / f\"ne_10m_admin_1_states_provinces_{datestamp}.zip\"\n",
    ")\n",
    "adm_url_fmt = (\n",
    "    \"https://biogeo.ucdavis.edu/data/gadm3.6/{ftype}/gadm36_{iso3}_{ftype}.zip\"\n",
    ")\n",
    "adm3_gpkg_fmt = \"gadm36_{iso3}.gpkg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_zip(url, out_path, overwrite=False):\n",
    "    out_path = Path(out_path)\n",
    "    if not out_path.exists() or overwrite:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "\n",
    "def process_gadm(in_gdf):\n",
    "    cols_to_load = [\"GID_0\", \"NAME_1\", \"NAME_2\", \"geometry\"]\n",
    "    col_map = {\"GID_0\": \"adm0_name\", \"NAME_1\": \"adm1_name\", \"NAME_2\": \"adm2_name\"}\n",
    "    if \"NAME_3\" in in_gdf.columns:\n",
    "        cols_to_load.append(\"NAME_3\")\n",
    "        col_map[\"NAME_3\"] = \"adm3_name\"\n",
    "\n",
    "    in_gdf = in_gdf[cols_to_load]\n",
    "    in_gdf = in_gdf.rename(columns=col_map)\n",
    "\n",
    "    cent = in_gdf[\"geometry\"].centroid\n",
    "    in_gdf[\"latitude\"] = cent.y\n",
    "    in_gdf[\"longitude\"] = cent.x\n",
    "\n",
    "    in_gdf = in_gdf.set_index([\"adm0_name\", \"adm1_name\", \"adm2_name\"])\n",
    "    if \"adm3_name\" in in_gdf.columns:\n",
    "        in_gdf = in_gdf.set_index(\"adm3_name\", append=True)\n",
    "\n",
    "    return in_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global adm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file\n",
    "adm1_url = \"https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip\"\n",
    "download_zip(adm1_url, adm1_shp_path, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process\n",
    "in_gdf = gpd.read_file(cutil.zipify_path(adm1_shp_path))\n",
    "adm_gdf = in_gdf[\n",
    "    [\"adm0_a3\", \"name\", \"geometry\", \"latitude\", \"longitude\", \"gadm_level\", \"name_alt\"]\n",
    "]\n",
    "adm_gdf = adm_gdf.rename(\n",
    "    columns={\"adm0_a3\": \"adm0_name\", \"name\": \"adm1_name\"}\n",
    ").set_index([\"adm0_name\", \"adm1_name\", \"gadm_level\"])\n",
    "\n",
    "# for now, when there are duplicates, just drop the second one without any better information\n",
    "# could not find a data dictionary for the shapefile\n",
    "adm_gdf = adm_gdf[~adm_gdf.index.duplicated(keep=\"first\")].reset_index(\n",
    "    drop=False, level=\"gadm_level\"\n",
    ")\n",
    "\n",
    "# we know france and italy are actually admin 2\n",
    "adm_gdf.loc[idx[[\"FRA\", \"ITA\"], :], \"gadm_level\"] = 2\n",
    "\n",
    "# separate into levels\n",
    "adm1_gdf = adm_gdf[adm_gdf.gadm_level == 1].drop(columns=\"gadm_level\")\n",
    "adm2_gdf = adm_gdf[adm_gdf.gadm_level == 2].drop(columns=\"gadm_level\")\n",
    "adm2_gdf.index = adm2_gdf.index.set_names(\"adm2_name\", level=\"adm1_name\")\n",
    "\n",
    "# Set up an adm3 dataset that is currently empty\n",
    "adm3_gdf = gpd.GeoDataFrame(\n",
    "    columns=adm2_gdf.reset_index(drop=False).columns, crs=adm_gdf.crs\n",
    ")\n",
    "adm3_gdf[\"adm3_name\"] = []\n",
    "adm3_gdf[\"adm1_name\"] = []\n",
    "adm3_gdf = adm3_gdf.set_index([\"adm0_name\", \"adm1_name\", \"adm2_name\", \"adm3_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adm2+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_fr_fpath = cutil.DATA / \"interim\" / \"france\" / \"departement_info.dta\"\n",
    "adm2_fr = pd.read_stata(\n",
    "    adm2_fr_fpath,\n",
    "    index_col=\"departement_name\",\n",
    "    columns=[\n",
    "        \"departement_name\",\n",
    "        \"adm1_name\",\n",
    "        \"cheflieu\",\n",
    "        \"densitehabitantskm2\",\n",
    "        \"superficiekmâ\",\n",
    "        \"population\",\n",
    "    ],\n",
    ")\n",
    "adm2_fr.index = adm2_fr.index.str.encode(\"ISO-8859-1\").str.decode(\"utf-8\")\n",
    "adm2_fr.cheflieu = adm2_fr.cheflieu.str.encode(\"ISO-8859-1\").str.decode(\"utf-8\")\n",
    "adm2_fr.index.name = \"adm2_name\"\n",
    "adm2_fr = adm2_fr.rename(\n",
    "    columns={\n",
    "        \"cheflieu\": \"capital\",\n",
    "        \"densitehabitantskm2\": \"pop_density_km2\",\n",
    "        \"superficiekmâ\": \"area_km2\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# manually correct some differences in naming btwn 2 datasets\n",
    "name_map = {\n",
    "    \"Guyane française\": \"Guyane\",\n",
    "    \"Haute-Rhin\": \"Haut-Rhin\",\n",
    "    \"Vendée\": \"Vandée\",\n",
    "    \"Côtes-d'Armor\": \"Côtes d'Armor\",\n",
    "    \"Seine-Saint-Denis\": \"Seine-St-Denis\",\n",
    "    \"Val-d'Oise\": \"Val-D'Oise\",\n",
    "    \"Seien-et-Marne\": \"Seine-et-Marne\",\n",
    "}\n",
    "adm2_gdf = adm2_gdf.rename(index=name_map, level=\"adm2_name\")\n",
    "\n",
    "# merge back in\n",
    "adm2_gdf = (\n",
    "    adm2_gdf.join(adm2_fr, on=\"adm2_name\", how=\"outer\")\n",
    "    .reset_index(drop=False)\n",
    "    .set_index([\"adm0_name\", \"adm1_name\", \"adm2_name\"])\n",
    ")\n",
    "\n",
    "# collapse to adm1 level and add that onto list\n",
    "adm1_fr = adm2_gdf.loc[[\"FRA\"], [\"geometry\", \"area_km2\", \"population\"]].dissolve(\n",
    "    by=[\"adm0_name\", \"adm1_name\"], aggfunc=\"sum\"\n",
    ")\n",
    "adm1_fr[\"latitude\"] = adm1_fr.geometry.centroid.y\n",
    "adm1_fr[\"longitude\"] = adm1_fr.geometry.centroid.x\n",
    "adm1_gdf = adm1_gdf.append(adm1_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these are from the same source but:\n",
    "- some work with the gpkg file others with the shapefile\n",
    "- some are adm3 some are adm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "isos = [\"ITA\", \"USA\", \"CHN\", \"KOR\", \"IRN\"]\n",
    "adm2_name_maps = {\n",
    "    \"ITA\": {\n",
    "        \"Firenze\": \"Florence\",\n",
    "        \"Reggio Emilia\": \"Reggio Nell'Emilia\",\n",
    "        \"Reggio Calabria\": \"Reggio Di Calabria\",\n",
    "        \"Pesaro e Urbino\": \"Pesaro E Urbino\",\n",
    "        \"Barletta-Andria Trani\": \"Barletta-Andria-Trani\",\n",
    "        \"Crotene\": \"Crotone\",\n",
    "        \"Aoste\": \"Aosta\",\n",
    "        \"Bozen\": \"Bolzano\",\n",
    "        \"Turin\": \"Torino\",\n",
    "        \"Padova\": \"Padua\",\n",
    "        \"Forlì-Cesena\": \"Forli' - Cesena\",\n",
    "        \"Siracusa\": \"Syracuse\",\n",
    "        \"Oristrano\": \"Oristano\",\n",
    "        \"Mantova\": \"Mantua\",\n",
    "        \"Monza e Brianza\": \"Monza and Brianza\",\n",
    "        \"Massa-Carrara\": \"Massa Carrara\",\n",
    "    }\n",
    "}\n",
    "for iso3 in isos:\n",
    "    # download if needed\n",
    "    if iso3 == \"CHN\":\n",
    "        ftype = \"shp\"\n",
    "    else:\n",
    "        ftype = \"gpkg\"\n",
    "    zip_path = cutil.get_adm_zip_path(iso3, datestamp)\n",
    "    if not zip_path.exists():\n",
    "        download_zip(adm_url_fmt.format(iso3=iso3, ftype=ftype), zip_path)\n",
    "    if ftype == \"gpkg\":\n",
    "        to_open = zip_path / f\"gadm36_{iso3}.gpkg\"\n",
    "    else:\n",
    "        to_open = zip_path / f\"gadm36_{iso3}_3.shp\"\n",
    "\n",
    "    # load gdf\n",
    "    in_gdf = process_gadm(gpd.read_file(cutil.zipify_path(to_open)))\n",
    "\n",
    "    if \"adm3_name\" in in_gdf.index.names:\n",
    "        adm3_gdf = adm3_gdf.append(in_gdf)\n",
    "\n",
    "        # now aggregate to level 2 to insert into that level\n",
    "        in_gdf = in_gdf.dissolve(by=[\"adm0_name\", \"adm1_name\", \"adm2_name\"])\n",
    "        in_gdf[\"latitude\"] = in_gdf.geometry.centroid.y\n",
    "        in_gdf[\"longitude\"] = in_gdf.geometry.centroid.x\n",
    "\n",
    "    # insert into level 2 dataset\n",
    "    if iso3 in adm2_gdf.index.get_level_values(\"adm0_name\").unique():\n",
    "        adm2_gdf = adm2_gdf.rename(index=adm2_name_maps[iso3], level=\"adm2_name\")\n",
    "        res = pd.merge(\n",
    "            adm2_gdf.loc[idx[iso3, :, :]],\n",
    "            in_gdf.reset_index(drop=False),\n",
    "            on=\"adm2_name\",\n",
    "            how=\"outer\",\n",
    "            indicator=True,\n",
    "        ).set_index([\"adm0_name\", \"adm1_name\", \"adm2_name\"])\n",
    "        assert (res._merge == \"both\").all()\n",
    "        del res[\"_merge\"]\n",
    "        for i in [\"geometry\", \"latitude\", \"longitude\"]:\n",
    "            res[i] = res[i + \"_y\"].fillna(res[i + \"_x\"])\n",
    "            res = res.drop(columns=[i + \"_x\", i + \"_y\"])\n",
    "        adm2_gdf = adm2_gdf.loc[\n",
    "            adm2_gdf.index.get_level_values(\"adm0_name\") != iso3\n",
    "        ].append(res)\n",
    "    else:\n",
    "        adm2_gdf = adm2_gdf.append(in_gdf)\n",
    "\n",
    "    # now aggregate to level 1 to replace that level with better/more consistent data\n",
    "    in_gdf = in_gdf.dissolve(by=[\"adm0_name\", \"adm1_name\"])\n",
    "    in_gdf[\"latitude\"] = in_gdf.geometry.centroid.y\n",
    "    in_gdf[\"longitude\"] = in_gdf.geometry.centroid.x\n",
    "    adm1_gdf = adm1_gdf[adm1_gdf.index.get_level_values(\"adm0_name\") != iso3]\n",
    "    adm1_gdf = adm1_gdf.append(in_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual name adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some manual adjustments to make this match with the naming of the data produced by country teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ianbolliger/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2857: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  result = self._run_cell(\n",
      "/Users/ianbolliger/miniconda3/lib/python3.8/site-packages/IPython/core/async_helpers.py:68: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  coro.send(None)\n"
     ]
    }
   ],
   "source": [
    "## get new regions/provinces\n",
    "region_dict = {\n",
    "    \"Emilia-Romagna\": \"Emilia Romagna\",\n",
    "    \"Friuli-Venezia Giulia\": \"Friuli Venezia Giulia\",\n",
    "    \"Apulia\": \"Puglia\",\n",
    "    \"Sicily\": \"Sicilia\",\n",
    "}\n",
    "add_regions = [\"P.A. Bolzano\", \"P.A. Trento\"]\n",
    "drop_regions = [\"Trentino-Alto Adige\"]\n",
    "province_dict = {\n",
    "    \"Forli' - Cesena\": \"Forlì-Cesena\",\n",
    "    \"Reggio Nell'Emilia\": \"Reggio nell'Emilia\",\n",
    "    \"Padua\": \"Padova\",\n",
    "    \"Reggio Di Calabria\": \"Reggio di Calabria\",\n",
    "    \"Pesaro E Urbino\": \"Pesaro e Urbino\",\n",
    "    \"Syracuse\": \"Siracusa\",\n",
    "    \"Florence\": \"Firenze\",\n",
    "    \"Mantua\": \"Mantova\",\n",
    "    \"Monza and Brianza\": \"Monza e della Brianza\",\n",
    "}\n",
    "add_provinces = [\"Sud Sardegna\"]\n",
    "add_provinces_reg = [\"Sardegna\"]\n",
    "n_reg = len(add_regions)\n",
    "new_reg = pd.DataFrame(\n",
    "    dict(adm0_name=[\"ITA\"] * n_reg, adm1_name=add_regions)\n",
    ").set_index([\"adm0_name\", \"adm1_name\"])\n",
    "n_prov = len(add_provinces)\n",
    "new_prov = pd.DataFrame(\n",
    "    dict(\n",
    "        adm0_name=[\"ITA\"] * n_prov, adm2_name=add_provinces, adm1_name=add_provinces_reg\n",
    "    )\n",
    ").set_index([\"adm0_name\", \"adm1_name\", \"adm2_name\"])\n",
    "\n",
    "\n",
    "## update regions for 2 provinces that are treated as\n",
    "## autonomous regions in the italy repo used for ITA_processed\n",
    "tmp = adm2_gdf.reset_index(level=\"adm1_name\", drop=False)\n",
    "for i in [\"Bolzano\", \"Trento\"]:\n",
    "    tmp.loc[idx[\"ITA\", i], \"adm1_name\"] = f\"P.A. {i}\"\n",
    "adm2_gdf = tmp.reset_index(drop=False).set_index(\n",
    "    [\"adm0_name\", \"adm1_name\", \"adm2_name\"]\n",
    ")\n",
    "\n",
    "\n",
    "## fix names\n",
    "adm1_gdf = adm1_gdf.rename(index=region_dict, level=\"adm1_name\")\n",
    "adm2_gdf = adm2_gdf.rename(index=region_dict, level=\"adm1_name\")\n",
    "adm3_gdf = adm3_gdf.rename(index=region_dict, level=\"adm1_name\")\n",
    "adm2_gdf = adm2_gdf.rename(index=province_dict, level=\"adm2_name\")\n",
    "adm3_gdf = adm3_gdf.rename(index=province_dict, level=\"adm2_name\")\n",
    "\n",
    "\n",
    "## split Trentino- into two provinces\n",
    "adm1_gdf = adm1_gdf.append(new_reg)\n",
    "adm1_gdf = adm1_gdf.drop(index=drop_regions, level=\"adm1_name\")\n",
    "\n",
    "\n",
    "## add additional province of sardegna\n",
    "adm2_gdf = adm2_gdf.append(new_prov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_apikey = \"24f4f2dc127d1386d07db9af73526aa052c9c41f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Census(census_apikey)\n",
    "pop_city = pd.DataFrame(\n",
    "    c.acs5.state_place((\"NAME\", \"B01003_001E\"), Census.ALL, Census.ALL)\n",
    ")\n",
    "pop_cty = pd.DataFrame(\n",
    "    c.acs5.state_county((\"NAME\", \"B01003_001E\"), Census.ALL, Census.ALL)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Place-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the place-level populations\n",
    "pop_city[[\"adm3_name\", \"adm_1_name\"]] = pd.DataFrame(\n",
    "    pop_city.NAME.str.split(\", \").values.tolist(), index=pop_city.index\n",
    ")\n",
    "pop_city = pop_city.rename(columns={\"B01003_001E\": \"pop\"}).drop(columns=\"NAME\")\n",
    "pop_city = pop_city.set_index([\"adm3_name\", \"adm_1_name\"])\n",
    "pop_city.to_csv(cutil.DATA / \"interim\" / \"usa\" / \"adm3_pop.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### County-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get county-level populations\n",
    "hasc_fips_url = \"http://www.statoids.com/yus.html\"\n",
    "# Create a handle, page, to handle the contents of the website\n",
    "page = requests.get(hasc_fips_url)\n",
    "\n",
    "# Store the contents of the website under doc\n",
    "doc = lh.fromstring(page.content)\n",
    "\n",
    "# Parse data\n",
    "tr_elements = doc.xpath('//*[@id=\"yui-main\"]/div/div/pre/text()[1]')\n",
    "row_list = tr_elements[0].split(\"\\r\\n\")[1:-1]\n",
    "headers = row_list[0].split()\n",
    "valid_rows = [r for r in row_list if r != \"\" and r[:4] not in [\"Name\", \"----\"]]\n",
    "name = [r[:23].rstrip() for r in valid_rows]\n",
    "t = [r[23] for r in valid_rows]\n",
    "hasc = [r[25:33] for r in valid_rows]\n",
    "fips = [r[34:39] for r in valid_rows]\n",
    "pop = [int(r[40:49].lstrip().replace(\",\", \"\")) for r in valid_rows]\n",
    "area_km2 = [int(r[50:57].lstrip().replace(\",\", \"\")) for r in valid_rows]\n",
    "area_mi2 = [int(r[58:65].lstrip().replace(\",\", \"\")) for r in valid_rows]\n",
    "z = [r[66] for r in valid_rows]\n",
    "capital = [r[68:] for r in valid_rows]\n",
    "\n",
    "# turn into dataframe\n",
    "us_county_df = pd.DataFrame(\n",
    "    {\n",
    "        \"name\": name,\n",
    "        \"type\": t,\n",
    "        \"hasc\": hasc,\n",
    "        \"fips\": fips,\n",
    "        \"population\": pop,\n",
    "        \"area_km2\": area_km2,\n",
    "        \"capital\": capital,\n",
    "    }\n",
    ").set_index(\"hasc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge in us adm2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_gdf = in_gdf = gpd.read_file(\n",
    "    cutil.zipify_path(cutil.get_adm_zip_path(\"USA\", datestamp) / \"gadm36_USA.gpkg\")\n",
    ")\n",
    "us_gdf = us_gdf[us_gdf.HASC_2.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_pops = us_gdf.join(us_county_df, on=\"HASC_2\", how=\"left\")\n",
    "us_pops = us_pops[[\"NAME_1\", \"NAME_2\", \"fips\", \"population\", \"area_km2\", \"capital\"]]\n",
    "us_pops = us_pops.rename(columns={\"NAME_1\": \"adm1_name\", \"NAME_2\": \"adm2_name\"})\n",
    "us_pops[\"pop_density_km2\"] = us_pops[\"population\"] / us_pops[\"area_km2\"]\n",
    "us_pops[\"adm0_name\"] = \"USA\"\n",
    "us_pops = us_pops.set_index([\"adm0_name\", \"adm1_name\", \"adm2_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge back into global adm datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this for France as well, b/c we haven't merged in adm2 pops to adm1 for france yet either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm2_gdf = adm2_gdf.fillna(us_pops)\n",
    "st_pops = (\n",
    "    adm2_gdf.loc[:, \"population\"].groupby([\"adm0_name\", \"adm1_name\"]).sum(min_count=1)\n",
    ")\n",
    "adm1_gdf[\"population\"] = adm1_gdf.population.fillna(st_pops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_fmt = \"http://demo.istat.it/pop2019/dati/{lvl}.zip\"\n",
    "ita_pop_dir = cutil.DATA_RAW / \"italy\" / \"population\"\n",
    "for u in [\"province\", \"regioni\", \"comuni\"]:\n",
    "    if not (ita_pop_dir / f\"{u}.csv\").exists():\n",
    "        download_and_extract(url_fmt.format(lvl=u), ita_pop_dir, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_provinces = {\n",
    "    \"Bolzano/Bozen\": \"Bolzano\",\n",
    "    \"Massa-Carrara\": \"Massa Carrara\",\n",
    "    \"Valle d'Aosta/Vallée d'Aoste\": \"Aosta\",\n",
    "}\n",
    "replace_regions = {\n",
    "    \"Emilia-Romagna\": \"Emilia Romagna\",\n",
    "    \"Friuli-Venezia Giulia\": \"Friuli Venezia Giulia\",\n",
    "    \"Valle d'Aosta/Vallée d'Aoste\": \"Valle d'Aosta\",\n",
    "    \"Bolzano\": \"P.A. Bolzano\",\n",
    "    \"Trento\": \"P.A. Trento\",\n",
    "}\n",
    "replace_munis = {\"Vo'\": \"Vò\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adm1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    ita_pop_dir / \"province.csv\",\n",
    "    skiprows=1,\n",
    "    usecols=[\"Provincia\", \"Totale Maschi\", \"Totale Femmine\", \"Età\"],\n",
    ")\n",
    "df[\"adm0_name\"] = \"ITA\"\n",
    "df = df.rename(columns={\"Provincia\": \"adm2_name\"}).set_index([\"adm0_name\", \"adm2_name\"])\n",
    "pop2 = df.loc[df[\"Età\"] == \"Totale\", [\"Totale Maschi\", \"Totale Femmine\"]].sum(axis=1)\n",
    "pop2 = pop2.rename(index=replace_provinces)\n",
    "pop2.name = \"population\"\n",
    "\n",
    "provinces_as_regions = pop2.loc[idx[:, [\"Bolzano\", \"Trento\"]]]\n",
    "pop2 = pop2.drop(index=[\"Bolzano\", \"Trento\"], level=\"adm2_name\")\n",
    "provinces_as_regions.index = provinces_as_regions.index.set_names(\n",
    "    \"adm1_name\", level=\"adm2_name\"\n",
    ")\n",
    "provinces_as_regions = provinces_as_regions.rename(\n",
    "    index=replace_regions, level=\"adm1_name\"\n",
    ")\n",
    "adm2_gdf.population = (\n",
    "    adm2_gdf.reset_index(level=\"adm1_name\").population.fillna(pop2).values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    ita_pop_dir / \"regioni.csv\",\n",
    "    skiprows=1,\n",
    "    usecols=[\"Regione\", \"Totale Maschi\", \"Totale Femmine\", \"Età\"],\n",
    ")\n",
    "df[\"adm0_name\"] = \"ITA\"\n",
    "df = df.rename(columns={\"Regione\": \"adm1_name\"}).set_index([\"adm0_name\", \"adm1_name\"])\n",
    "pop1 = df.loc[df[\"Età\"] == \"Totale\", [\"Totale Maschi\", \"Totale Femmine\"]].sum(axis=1)\n",
    "pop1 = pop1.rename(index=replace_regions)\n",
    "pop1.name = \"population\"\n",
    "\n",
    "pop1 = pop1.append(provinces_as_regions)\n",
    "adm1_gdf.population = adm1_gdf.population.fillna(pop1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    ita_pop_dir / \"comuni.csv\",\n",
    "    skiprows=1,\n",
    "    usecols=[\"Denominazione\", \"Totale Maschi\", \"Totale Femmine\", \"Età\"],\n",
    ")\n",
    "df[\"adm0_name\"] = \"ITA\"\n",
    "df = df.rename(columns={\"Denominazione\": \"adm3_name\"}).set_index(\n",
    "    [\"adm0_name\", \"adm3_name\"]\n",
    ")\n",
    "pop3 = df.loc[df[\"Età\"] == 999, [\"Totale Maschi\", \"Totale Femmine\"]].sum(axis=1)\n",
    "pop3.name = \"population\"\n",
    "\n",
    "# don't know what to do with same-named cities so we'll just keep those pops as missing\n",
    "# except Vo'Eugane which is used for pop weighting\n",
    "this_city = pop3.loc[idx[:, [\"Vo'\"]]].rename(index={\"Vo'\": \"Vo'Eugane\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    ita_pop_dir / \"comuni.csv\",\n",
    "    skiprows=1,\n",
    "    usecols=[\"Denominazione\", \"Totale Maschi\", \"Totale Femmine\", \"Età\"],\n",
    ")\n",
    "df[\"adm0_name\"] = \"ITA\"\n",
    "df = df.rename(columns={\"Denominazione\": \"adm3_name\"}).set_index(\n",
    "    [\"adm0_name\", \"adm3_name\"]\n",
    ")\n",
    "pop3 = df.loc[df[\"Età\"] == 999, [\"Totale Maschi\", \"Totale Femmine\"]].sum(axis=1)\n",
    "pop3.name = \"population\"\n",
    "pop3 = pop3.rename(index=replace_munis)\n",
    "\n",
    "\n",
    "## making sure we match the important cities (ones that are used in pop weighting)\n",
    "adm3_gdf = adm3_gdf.rename(\n",
    "    lambda x: x.replace(\"d' Adda\", \"d'Adda\").replace(\n",
    "        \"Terranova Dei Passerini\", \"Terranova dei Passerini\"\n",
    "    ),\n",
    "    level=\"adm3_name\",\n",
    ")\n",
    "\n",
    "# these two municipalities merged\n",
    "castel = gpd.GeoDataFrame(\n",
    "    adm3_gdf.loc[idx[:, :, :, [\"Cavacurta\", \"Camairago\"]], [\"geometry\"]]\n",
    ").dissolve(by=[\"adm0_name\", \"adm1_name\", \"adm2_name\"])\n",
    "castel[\"adm3_name\"] = [\"Castelgerundo\"]\n",
    "castel[\"latitude\"] = castel.geometry.centroid.y\n",
    "castel[\"longitude\"] = castel.geometry.centroid.x\n",
    "castel = castel.set_index(\"adm3_name\", append=True)\n",
    "adm3_gdf = adm3_gdf[\n",
    "    ~adm3_gdf.index.get_level_values(\"adm3_name\").isin([\"Cavacurta\", \"Camairago\"])\n",
    "].append(castel)\n",
    "\n",
    "\n",
    "## don't know what to do with same-named cities so we'll just keep those pops as missing\n",
    "pop3 = pop3[~pop3.index.duplicated(keep=False)]\n",
    "\n",
    "\n",
    "## merge\n",
    "adm3_gdf = (\n",
    "    adm3_gdf.reset_index(level=[\"adm1_name\", \"adm2_name\"], drop=False)\n",
    "    .join(pop3, how=\"left\")\n",
    "    .reset_index(drop=False)\n",
    "    .set_index([\"adm0_name\", \"adm1_name\", \"adm2_name\", \"adm3_name\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "irn_url = r\"https://www.citypopulation.de/en/iran/admin/\"\n",
    "r = requests.get(irn_url)\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data, \"lxml\")\n",
    "table = soup.table\n",
    "\n",
    "adm1s = table.find_all(\"tbody\", {\"class\": \"admin1\"})\n",
    "adm2s = table.find_all(\"tbody\", {\"class\": \"admin2\"})\n",
    "\n",
    "# just want name and latest census pop\n",
    "adm1_rows = []\n",
    "for a in adm1s:\n",
    "    rows = a.find_all(\"tr\")\n",
    "    for r in rows:\n",
    "        td = r.find_all(\"td\")\n",
    "        row = [i.text for i in td]\n",
    "        adm1_rows.append([row[0], int(row[-2].replace(\",\", \"\"))])\n",
    "adm1_irn = pd.DataFrame(adm1_rows, columns=[\"adm1_name\", \"population\"]).set_index(\n",
    "    \"adm1_name\"\n",
    ")\n",
    "\n",
    "adm2_rows = []\n",
    "for a in adm2s:\n",
    "    # complicated way to get province from previous admin1 level\n",
    "    prov = \"\".join(list(a.previous_sibling.previous_sibling.strings)[1:-6])\n",
    "    rows = a.find_all(\"tr\")\n",
    "    for r in rows:\n",
    "        td = r.find_all(\"td\")\n",
    "        row = [i.text for i in td]\n",
    "        adm2_rows.append([prov, row[0], int(row[-2].replace(\",\", \"\"))])\n",
    "adm2_irn = pd.DataFrame(\n",
    "    adm2_rows, columns=[\"adm1_name\", \"adm2_name\", \"population\"]\n",
    ").set_index([\"adm1_name\", \"adm2_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker(wrong_options, correct_options):\n",
    "    \"\"\"Fuzzy matching for names\"\"\"\n",
    "    names_array = []\n",
    "    ratio_array = []\n",
    "    for wrong_option in wrong_options:\n",
    "        if wrong_option in correct_options:\n",
    "            names_array.append(wrong_option)\n",
    "            ratio_array.append(\"100\")\n",
    "        else:\n",
    "            x = process.extractOne(\n",
    "                wrong_option, correct_options, scorer=fuzz.token_set_ratio\n",
    "            )\n",
    "            names_array.append(x[0])\n",
    "            ratio_array.append(x[1])\n",
    "    return names_array, ratio_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_orig = adm1_irn.index.values\n",
    "cleaned_names = checker(\n",
    "    adm1_orig, adm1_gdf.loc[idx[\"IRN\"]].index.get_level_values(\"adm1_name\")\n",
    ")[0]\n",
    "\n",
    "# I know that the 3rd one is mapping to East rather than West :(\n",
    "cleaned_names[2] = \"West Azarbaijan\"\n",
    "\n",
    "adm1_irn[\"adm1_name\"] = cleaned_names\n",
    "adm1_irn[\"adm0_name\"] = \"IRN\"\n",
    "adm1_irn = adm1_irn.set_index([\"adm0_name\", \"adm1_name\"], drop=True)\n",
    "\n",
    "# merge in pops\n",
    "adm1_gdf.population = adm1_gdf.population.fillna(adm1_irn.population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's going to be some challenges in fuzzy merging adm2 level populations, but we're not running analyses on adm2 yet, so I'm holding off on this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finishing_touches(df):\n",
    "    # area\n",
    "    area_km2_mercator = df[df.geometry.notna()].to_crs(\"EPSG:3395\").geometry.area / 1e6\n",
    "    if \"area_km2\" in df.columns:\n",
    "        df[\"area_km2\"] = df.area_km2.fillna(area_km2_mercator)\n",
    "    else:\n",
    "        df[\"area_km2\"] = area_km2_mercator\n",
    "\n",
    "    # pop density\n",
    "    if \"pop_density_km2\" in df.columns:\n",
    "        df.pop_density_km2 = df.pop_density_km2.fillna(\n",
    "            df.population.astype(float) / df.area_km2\n",
    "        )\n",
    "    else:\n",
    "        df[\"pop_density_km2\"] = df.population.astype(float) / df.area_km2\n",
    "\n",
    "    # lat/lon\n",
    "    df.longitude = df.longitude.fillna(df.geometry.centroid.x)\n",
    "    df.latitude = df.latitude.fillna(df.geometry.centroid.y)\n",
    "\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "adm1_gdf = finishing_touches(adm1_gdf)\n",
    "adm2_gdf = finishing_touches(adm2_gdf)\n",
    "adm3_gdf = finishing_touches(adm3_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, i in enumerate([adm1_gdf, adm2_gdf, adm3_gdf]):\n",
    "    fname = f\"adm{ix+1}\"\n",
    "    out_dir = cutil.DATA_INTERIM / \"adm\" / fname\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    i.to_file(out_dir / f\"{fname}.shp\", index=True)\n",
    "    i.drop(columns=\"geometry\").to_csv(out_dir / f\"{fname}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
